{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmaQtuvLx6QN"
   },
   "source": [
    "# Recommender Systems\n",
    "\n",
    "#### Author: Juan Gordyn\n",
    "\n",
    "## I. Introduction\n",
    "The aim of this task is to use data from Flickr to recommend photos to a set of given users. In order to achieve this, we are going to go through the following steps:\n",
    "* Importing the libraries and loading the data.\n",
    "* Building, evaluating and comparing 3 different basic Neural Networks models: Matrix Factorization (MF), Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP). After the comparison we are going to keep only one of these for further tuning, which, as we will see, is going to be the simplest: MF.\n",
    "* Having decided to continue with MF, we are going to construct a new model using the given users and items data: MF_UI (UI for User-Item) and another one using all the extra data: users, items, and social links data: MF_UI_LINKS. We will proceed to evaluate both models, compare them and keep only one for further steps.\n",
    "* We will add biases to the chosen model of the previous step (MF_UI, as we will see) and play around with the hyperparameter negative_ratio, which refers to number_of_negative_samples/number_of_positive_samples in the training data (negative samples being people not interacting with an image, which is synthetically added to the data and positive samples being people interacting, which is information given by the original data).\n",
    "* We will choose the best-performing model from the previous step and play around with the hyperparameter weight of decay, which is responsible for performing L2 regularization.\n",
    "* Making predictions on the test data using the model with the best performance over all the models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGv8PYUH4bNc"
   },
   "source": [
    "## II. Importing libraries and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82fJfP4H-oVa",
    "outputId": "e669527e-7a42-4d03-d2f2-46b66069a5ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJn_CNDL-9NB"
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import heapq\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from time import time\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tPAgzdKNUqQ"
   },
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "LCb19Y3l_Nn3",
    "outputId": "180f389a-c98e-4ebc-8edc-4338b1e0c2f3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        0        0       1\n",
       "1        0        1       1\n",
       "2        0        2       1\n",
       "3        0        3       1\n",
       "4        0        4       1"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('/content/drive/My Drive/res2021/flickr_train_data.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPXb7BZ147Yp",
    "outputId": "c8616ea5-d642-4771-ee37-beabe58cbed8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    110129\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMO5KMf74k1K"
   },
   "source": [
    "We can see how the training data has only positive instances (ones in the rating column). This is where the negative_ratio hyperparameter will become handy, to add items the user has not interacted with.\n",
    "\n",
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "1jqGwzD9_1SC",
    "outputId": "3c115638-6c4d-4edc-d029-b110399879a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>8821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id\n",
       "0        0     8929\n",
       "1        0     8906\n",
       "2        0     8838\n",
       "3        0     8821\n",
       "4        0     8756"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('/content/drive/My Drive/res2021/flickr_test_data.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUdZX7o75CH4"
   },
   "source": [
    "Test data just gives as user_ids and a set of item_ids to choose from which we will have to rank in order to make the recommendations for each user.\n",
    "\n",
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "gca2Z4L0_75e",
    "outputId": "5d40716d-ef57-4080-d0ba-bfb8e88a7cf3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5425</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8631</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        0       20       1\n",
       "1        0     3260       0\n",
       "2        0      390       0\n",
       "3        0     5425       0\n",
       "4        0     8631       0"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = pd.read_csv('/content/drive/My Drive/res2021/flickr_validation_data.csv')\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlAPOd2P5Tvc"
   },
   "source": [
    "Validation data looks exactly the same as training data. We are going to use it to be able to compare between models as to understand which one we should keep.\n",
    "\n",
    "### Users pre-trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "-VgCrjlaAF5X",
    "outputId": "894eeeed-0838-42f8-8282-4f5db5b6a7de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fea_0</th>\n",
       "      <th>fea_1</th>\n",
       "      <th>fea_2</th>\n",
       "      <th>fea_3</th>\n",
       "      <th>fea_4</th>\n",
       "      <th>fea_5</th>\n",
       "      <th>fea_6</th>\n",
       "      <th>fea_7</th>\n",
       "      <th>fea_8</th>\n",
       "      <th>fea_9</th>\n",
       "      <th>fea_10</th>\n",
       "      <th>fea_11</th>\n",
       "      <th>fea_12</th>\n",
       "      <th>fea_13</th>\n",
       "      <th>fea_14</th>\n",
       "      <th>fea_15</th>\n",
       "      <th>fea_16</th>\n",
       "      <th>fea_17</th>\n",
       "      <th>fea_18</th>\n",
       "      <th>fea_19</th>\n",
       "      <th>fea_20</th>\n",
       "      <th>fea_21</th>\n",
       "      <th>fea_22</th>\n",
       "      <th>fea_23</th>\n",
       "      <th>fea_24</th>\n",
       "      <th>fea_25</th>\n",
       "      <th>fea_26</th>\n",
       "      <th>fea_27</th>\n",
       "      <th>fea_28</th>\n",
       "      <th>fea_29</th>\n",
       "      <th>fea_30</th>\n",
       "      <th>fea_31</th>\n",
       "      <th>fea_32</th>\n",
       "      <th>fea_33</th>\n",
       "      <th>fea_34</th>\n",
       "      <th>fea_35</th>\n",
       "      <th>fea_36</th>\n",
       "      <th>fea_37</th>\n",
       "      <th>fea_38</th>\n",
       "      <th>...</th>\n",
       "      <th>fea_216</th>\n",
       "      <th>fea_217</th>\n",
       "      <th>fea_218</th>\n",
       "      <th>fea_219</th>\n",
       "      <th>fea_220</th>\n",
       "      <th>fea_221</th>\n",
       "      <th>fea_222</th>\n",
       "      <th>fea_223</th>\n",
       "      <th>fea_224</th>\n",
       "      <th>fea_225</th>\n",
       "      <th>fea_226</th>\n",
       "      <th>fea_227</th>\n",
       "      <th>fea_228</th>\n",
       "      <th>fea_229</th>\n",
       "      <th>fea_230</th>\n",
       "      <th>fea_231</th>\n",
       "      <th>fea_232</th>\n",
       "      <th>fea_233</th>\n",
       "      <th>fea_234</th>\n",
       "      <th>fea_235</th>\n",
       "      <th>fea_236</th>\n",
       "      <th>fea_237</th>\n",
       "      <th>fea_238</th>\n",
       "      <th>fea_239</th>\n",
       "      <th>fea_240</th>\n",
       "      <th>fea_241</th>\n",
       "      <th>fea_242</th>\n",
       "      <th>fea_243</th>\n",
       "      <th>fea_244</th>\n",
       "      <th>fea_245</th>\n",
       "      <th>fea_246</th>\n",
       "      <th>fea_247</th>\n",
       "      <th>fea_248</th>\n",
       "      <th>fea_249</th>\n",
       "      <th>fea_250</th>\n",
       "      <th>fea_251</th>\n",
       "      <th>fea_252</th>\n",
       "      <th>fea_253</th>\n",
       "      <th>fea_254</th>\n",
       "      <th>fea_255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.238114</td>\n",
       "      <td>-1.020155</td>\n",
       "      <td>-1.370791</td>\n",
       "      <td>-1.892076</td>\n",
       "      <td>-1.615505</td>\n",
       "      <td>-1.056106</td>\n",
       "      <td>-1.260189</td>\n",
       "      <td>-1.537514</td>\n",
       "      <td>-1.279543</td>\n",
       "      <td>-1.906656</td>\n",
       "      <td>-1.659302</td>\n",
       "      <td>-1.925202</td>\n",
       "      <td>-1.515665</td>\n",
       "      <td>-1.559344</td>\n",
       "      <td>-1.473872</td>\n",
       "      <td>-1.481108</td>\n",
       "      <td>-0.928755</td>\n",
       "      <td>-1.867626</td>\n",
       "      <td>-1.564498</td>\n",
       "      <td>-1.415489</td>\n",
       "      <td>-1.361105</td>\n",
       "      <td>-1.760134</td>\n",
       "      <td>-1.593500</td>\n",
       "      <td>-1.483804</td>\n",
       "      <td>-1.568468</td>\n",
       "      <td>-1.537735</td>\n",
       "      <td>-1.870094</td>\n",
       "      <td>-1.703073</td>\n",
       "      <td>-1.510810</td>\n",
       "      <td>-1.891596</td>\n",
       "      <td>-1.635605</td>\n",
       "      <td>-1.580935</td>\n",
       "      <td>-1.621338</td>\n",
       "      <td>-2.106059</td>\n",
       "      <td>-1.747817</td>\n",
       "      <td>-1.253811</td>\n",
       "      <td>-1.582972</td>\n",
       "      <td>-1.902225</td>\n",
       "      <td>-1.767316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.990106</td>\n",
       "      <td>-1.173747</td>\n",
       "      <td>-1.334480</td>\n",
       "      <td>-1.832711</td>\n",
       "      <td>-2.000400</td>\n",
       "      <td>-1.326554</td>\n",
       "      <td>-1.343949</td>\n",
       "      <td>-1.679905</td>\n",
       "      <td>-0.949615</td>\n",
       "      <td>-1.271077</td>\n",
       "      <td>-1.887741</td>\n",
       "      <td>-1.525564</td>\n",
       "      <td>-1.696560</td>\n",
       "      <td>-1.109712</td>\n",
       "      <td>-1.176654</td>\n",
       "      <td>-1.433217</td>\n",
       "      <td>-1.148363</td>\n",
       "      <td>-1.317346</td>\n",
       "      <td>-1.368154</td>\n",
       "      <td>-1.521333</td>\n",
       "      <td>-1.136240</td>\n",
       "      <td>-1.308951</td>\n",
       "      <td>-1.418725</td>\n",
       "      <td>-1.583235</td>\n",
       "      <td>-1.684002</td>\n",
       "      <td>-1.112810</td>\n",
       "      <td>-1.090714</td>\n",
       "      <td>-1.876829</td>\n",
       "      <td>-1.927204</td>\n",
       "      <td>-1.956156</td>\n",
       "      <td>-1.639152</td>\n",
       "      <td>-1.351084</td>\n",
       "      <td>-1.526210</td>\n",
       "      <td>-1.441551</td>\n",
       "      <td>-1.567562</td>\n",
       "      <td>-1.363613</td>\n",
       "      <td>-1.133677</td>\n",
       "      <td>-1.448653</td>\n",
       "      <td>-1.912987</td>\n",
       "      <td>-1.220462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.142714</td>\n",
       "      <td>-1.012310</td>\n",
       "      <td>-1.109844</td>\n",
       "      <td>-1.806807</td>\n",
       "      <td>-1.533666</td>\n",
       "      <td>-0.894846</td>\n",
       "      <td>-1.360289</td>\n",
       "      <td>-1.159866</td>\n",
       "      <td>-1.184760</td>\n",
       "      <td>-1.717158</td>\n",
       "      <td>-1.617885</td>\n",
       "      <td>-1.877182</td>\n",
       "      <td>-1.388428</td>\n",
       "      <td>-1.478378</td>\n",
       "      <td>-1.378944</td>\n",
       "      <td>-1.167055</td>\n",
       "      <td>-0.836044</td>\n",
       "      <td>-1.735027</td>\n",
       "      <td>-1.372658</td>\n",
       "      <td>-1.196414</td>\n",
       "      <td>-1.064743</td>\n",
       "      <td>-1.550732</td>\n",
       "      <td>-1.526151</td>\n",
       "      <td>-1.188836</td>\n",
       "      <td>-1.597194</td>\n",
       "      <td>-1.342873</td>\n",
       "      <td>-1.600191</td>\n",
       "      <td>-1.589119</td>\n",
       "      <td>-1.304950</td>\n",
       "      <td>-1.666081</td>\n",
       "      <td>-1.761278</td>\n",
       "      <td>-1.427513</td>\n",
       "      <td>-1.628702</td>\n",
       "      <td>-2.081269</td>\n",
       "      <td>-1.715110</td>\n",
       "      <td>-1.150502</td>\n",
       "      <td>-1.401053</td>\n",
       "      <td>-1.567127</td>\n",
       "      <td>-1.722585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.822849</td>\n",
       "      <td>-1.003970</td>\n",
       "      <td>-1.261567</td>\n",
       "      <td>-1.763957</td>\n",
       "      <td>-2.015746</td>\n",
       "      <td>-1.219497</td>\n",
       "      <td>-1.257548</td>\n",
       "      <td>-1.504811</td>\n",
       "      <td>-0.840864</td>\n",
       "      <td>-1.101200</td>\n",
       "      <td>-1.708111</td>\n",
       "      <td>-1.396081</td>\n",
       "      <td>-1.519199</td>\n",
       "      <td>-1.010751</td>\n",
       "      <td>-1.112336</td>\n",
       "      <td>-1.185319</td>\n",
       "      <td>-0.981793</td>\n",
       "      <td>-1.000713</td>\n",
       "      <td>-1.253462</td>\n",
       "      <td>-1.188320</td>\n",
       "      <td>-1.114223</td>\n",
       "      <td>-1.116385</td>\n",
       "      <td>-1.417270</td>\n",
       "      <td>-1.553726</td>\n",
       "      <td>-1.724767</td>\n",
       "      <td>-1.131972</td>\n",
       "      <td>-0.725171</td>\n",
       "      <td>-1.862194</td>\n",
       "      <td>-1.888966</td>\n",
       "      <td>-1.481360</td>\n",
       "      <td>-1.471417</td>\n",
       "      <td>-1.028792</td>\n",
       "      <td>-1.397979</td>\n",
       "      <td>-1.304754</td>\n",
       "      <td>-1.630009</td>\n",
       "      <td>-1.318048</td>\n",
       "      <td>-1.080598</td>\n",
       "      <td>-1.251735</td>\n",
       "      <td>-1.813930</td>\n",
       "      <td>-0.975978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.698521</td>\n",
       "      <td>-1.605196</td>\n",
       "      <td>-1.328976</td>\n",
       "      <td>-1.492204</td>\n",
       "      <td>-1.541542</td>\n",
       "      <td>-1.155632</td>\n",
       "      <td>-1.297039</td>\n",
       "      <td>-1.485625</td>\n",
       "      <td>-1.348223</td>\n",
       "      <td>-1.573296</td>\n",
       "      <td>-1.435555</td>\n",
       "      <td>-1.781028</td>\n",
       "      <td>-1.215887</td>\n",
       "      <td>-1.781181</td>\n",
       "      <td>-1.263028</td>\n",
       "      <td>-1.381850</td>\n",
       "      <td>-0.990998</td>\n",
       "      <td>-1.495829</td>\n",
       "      <td>-1.439823</td>\n",
       "      <td>-1.307382</td>\n",
       "      <td>-1.474037</td>\n",
       "      <td>-1.614065</td>\n",
       "      <td>-1.349969</td>\n",
       "      <td>-1.499975</td>\n",
       "      <td>-1.689617</td>\n",
       "      <td>-1.398021</td>\n",
       "      <td>-1.619859</td>\n",
       "      <td>-1.545190</td>\n",
       "      <td>-1.684246</td>\n",
       "      <td>-1.546842</td>\n",
       "      <td>-1.617103</td>\n",
       "      <td>-1.799428</td>\n",
       "      <td>-1.430386</td>\n",
       "      <td>-1.868050</td>\n",
       "      <td>-1.501291</td>\n",
       "      <td>-1.378281</td>\n",
       "      <td>-1.786608</td>\n",
       "      <td>-1.532645</td>\n",
       "      <td>-1.935686</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.214127</td>\n",
       "      <td>-1.073859</td>\n",
       "      <td>-1.535632</td>\n",
       "      <td>-2.007961</td>\n",
       "      <td>-1.916732</td>\n",
       "      <td>-1.504394</td>\n",
       "      <td>-1.615253</td>\n",
       "      <td>-1.498708</td>\n",
       "      <td>-1.054974</td>\n",
       "      <td>-1.395028</td>\n",
       "      <td>-1.811057</td>\n",
       "      <td>-1.198182</td>\n",
       "      <td>-1.746893</td>\n",
       "      <td>-1.126945</td>\n",
       "      <td>-1.526381</td>\n",
       "      <td>-1.099525</td>\n",
       "      <td>-1.168397</td>\n",
       "      <td>-1.261002</td>\n",
       "      <td>-1.218141</td>\n",
       "      <td>-1.676121</td>\n",
       "      <td>-1.444680</td>\n",
       "      <td>-1.112782</td>\n",
       "      <td>-1.159583</td>\n",
       "      <td>-1.555021</td>\n",
       "      <td>-1.458171</td>\n",
       "      <td>-1.340580</td>\n",
       "      <td>-1.187551</td>\n",
       "      <td>-1.785969</td>\n",
       "      <td>-1.566249</td>\n",
       "      <td>-1.646420</td>\n",
       "      <td>-1.529016</td>\n",
       "      <td>-1.372415</td>\n",
       "      <td>-1.769800</td>\n",
       "      <td>-1.414428</td>\n",
       "      <td>-1.550462</td>\n",
       "      <td>-1.249593</td>\n",
       "      <td>-1.284915</td>\n",
       "      <td>-1.446422</td>\n",
       "      <td>-1.686583</td>\n",
       "      <td>-1.145916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.328761</td>\n",
       "      <td>-1.267363</td>\n",
       "      <td>-1.243220</td>\n",
       "      <td>-1.721551</td>\n",
       "      <td>-1.182082</td>\n",
       "      <td>-1.320123</td>\n",
       "      <td>-1.115170</td>\n",
       "      <td>-1.122703</td>\n",
       "      <td>-1.140961</td>\n",
       "      <td>-1.643231</td>\n",
       "      <td>-1.374062</td>\n",
       "      <td>-1.625373</td>\n",
       "      <td>-1.051756</td>\n",
       "      <td>-1.420016</td>\n",
       "      <td>-1.304729</td>\n",
       "      <td>-1.253752</td>\n",
       "      <td>-0.750538</td>\n",
       "      <td>-1.559732</td>\n",
       "      <td>-1.440565</td>\n",
       "      <td>-1.109614</td>\n",
       "      <td>-1.119513</td>\n",
       "      <td>-1.326863</td>\n",
       "      <td>-1.607983</td>\n",
       "      <td>-1.418027</td>\n",
       "      <td>-1.492478</td>\n",
       "      <td>-1.216149</td>\n",
       "      <td>-1.244658</td>\n",
       "      <td>-1.357094</td>\n",
       "      <td>-1.399642</td>\n",
       "      <td>-1.350963</td>\n",
       "      <td>-1.461319</td>\n",
       "      <td>-1.232699</td>\n",
       "      <td>-1.705400</td>\n",
       "      <td>-1.726151</td>\n",
       "      <td>-1.475964</td>\n",
       "      <td>-1.015996</td>\n",
       "      <td>-1.430632</td>\n",
       "      <td>-1.612268</td>\n",
       "      <td>-1.477600</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.266713</td>\n",
       "      <td>-1.098897</td>\n",
       "      <td>-1.240819</td>\n",
       "      <td>-1.702107</td>\n",
       "      <td>-1.740631</td>\n",
       "      <td>-1.323688</td>\n",
       "      <td>-1.375717</td>\n",
       "      <td>-1.217594</td>\n",
       "      <td>-1.166748</td>\n",
       "      <td>-1.541737</td>\n",
       "      <td>-1.719531</td>\n",
       "      <td>-1.428146</td>\n",
       "      <td>-1.405180</td>\n",
       "      <td>-1.113524</td>\n",
       "      <td>-1.239969</td>\n",
       "      <td>-1.179385</td>\n",
       "      <td>-1.413361</td>\n",
       "      <td>-1.066098</td>\n",
       "      <td>-1.308499</td>\n",
       "      <td>-1.310713</td>\n",
       "      <td>-1.116245</td>\n",
       "      <td>-1.071725</td>\n",
       "      <td>-1.218907</td>\n",
       "      <td>-1.160358</td>\n",
       "      <td>-1.400148</td>\n",
       "      <td>-1.196517</td>\n",
       "      <td>-0.917711</td>\n",
       "      <td>-1.528373</td>\n",
       "      <td>-1.519907</td>\n",
       "      <td>-1.555350</td>\n",
       "      <td>-1.441997</td>\n",
       "      <td>-0.956042</td>\n",
       "      <td>-1.570591</td>\n",
       "      <td>-1.238835</td>\n",
       "      <td>-1.489867</td>\n",
       "      <td>-1.320204</td>\n",
       "      <td>-1.202990</td>\n",
       "      <td>-1.322884</td>\n",
       "      <td>-1.580290</td>\n",
       "      <td>-0.931394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.132466</td>\n",
       "      <td>-1.026957</td>\n",
       "      <td>-1.019664</td>\n",
       "      <td>-1.649181</td>\n",
       "      <td>-1.382753</td>\n",
       "      <td>-0.776261</td>\n",
       "      <td>-1.221357</td>\n",
       "      <td>-1.061034</td>\n",
       "      <td>-1.041313</td>\n",
       "      <td>-1.600397</td>\n",
       "      <td>-1.532712</td>\n",
       "      <td>-1.515810</td>\n",
       "      <td>-1.130584</td>\n",
       "      <td>-1.291163</td>\n",
       "      <td>-1.162265</td>\n",
       "      <td>-1.167415</td>\n",
       "      <td>-0.733019</td>\n",
       "      <td>-1.438941</td>\n",
       "      <td>-1.196358</td>\n",
       "      <td>-1.018312</td>\n",
       "      <td>-0.927579</td>\n",
       "      <td>-1.494626</td>\n",
       "      <td>-1.410698</td>\n",
       "      <td>-1.007854</td>\n",
       "      <td>-1.245793</td>\n",
       "      <td>-1.375546</td>\n",
       "      <td>-1.428240</td>\n",
       "      <td>-1.381729</td>\n",
       "      <td>-1.157630</td>\n",
       "      <td>-1.422396</td>\n",
       "      <td>-1.452377</td>\n",
       "      <td>-1.368716</td>\n",
       "      <td>-1.439233</td>\n",
       "      <td>-1.780620</td>\n",
       "      <td>-1.543544</td>\n",
       "      <td>-0.925612</td>\n",
       "      <td>-1.242800</td>\n",
       "      <td>-1.393965</td>\n",
       "      <td>-1.599913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.735095</td>\n",
       "      <td>-0.825063</td>\n",
       "      <td>-1.117274</td>\n",
       "      <td>-1.646374</td>\n",
       "      <td>-1.709945</td>\n",
       "      <td>-1.034121</td>\n",
       "      <td>-1.120539</td>\n",
       "      <td>-1.356616</td>\n",
       "      <td>-0.789711</td>\n",
       "      <td>-1.101305</td>\n",
       "      <td>-1.518591</td>\n",
       "      <td>-1.182465</td>\n",
       "      <td>-1.416471</td>\n",
       "      <td>-0.866871</td>\n",
       "      <td>-1.004829</td>\n",
       "      <td>-1.108024</td>\n",
       "      <td>-0.925540</td>\n",
       "      <td>-0.839730</td>\n",
       "      <td>-1.150537</td>\n",
       "      <td>-1.091620</td>\n",
       "      <td>-1.103983</td>\n",
       "      <td>-0.869280</td>\n",
       "      <td>-1.286227</td>\n",
       "      <td>-1.347098</td>\n",
       "      <td>-1.515295</td>\n",
       "      <td>-1.046581</td>\n",
       "      <td>-0.744637</td>\n",
       "      <td>-1.728436</td>\n",
       "      <td>-1.647484</td>\n",
       "      <td>-1.528122</td>\n",
       "      <td>-1.307160</td>\n",
       "      <td>-0.940088</td>\n",
       "      <td>-1.313026</td>\n",
       "      <td>-1.308773</td>\n",
       "      <td>-1.455208</td>\n",
       "      <td>-1.056440</td>\n",
       "      <td>-0.950118</td>\n",
       "      <td>-1.029165</td>\n",
       "      <td>-1.593675</td>\n",
       "      <td>-0.886145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     fea_0     fea_1  ...   fea_253   fea_254   fea_255\n",
       "0           0 -1.238114 -1.020155  ... -1.448653 -1.912987 -1.220462\n",
       "1           1 -1.142714 -1.012310  ... -1.251735 -1.813930 -0.975978\n",
       "2           2 -1.698521 -1.605196  ... -1.446422 -1.686583 -1.145916\n",
       "3           3 -1.328761 -1.267363  ... -1.322884 -1.580290 -0.931394\n",
       "4           4 -1.132466 -1.026957  ... -1.029165 -1.593675 -0.886145\n",
       "\n",
       "[5 rows x 257 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df = pd.read_csv('/content/drive/My Drive/res2021/flickr_user_fea.csv')\n",
    "users_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgAhxu8D5iyh"
   },
   "source": [
    "Users pre-trained data that we could use as input to boost our model's performance.\n",
    "\n",
    "### Items pre-trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "0-fW45I8BELn",
    "outputId": "b6f63989-41ff-424d-c794-92d542528bb1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fea_0</th>\n",
       "      <th>fea_1</th>\n",
       "      <th>fea_2</th>\n",
       "      <th>fea_3</th>\n",
       "      <th>fea_4</th>\n",
       "      <th>fea_5</th>\n",
       "      <th>fea_6</th>\n",
       "      <th>fea_7</th>\n",
       "      <th>fea_8</th>\n",
       "      <th>fea_9</th>\n",
       "      <th>fea_10</th>\n",
       "      <th>fea_11</th>\n",
       "      <th>fea_12</th>\n",
       "      <th>fea_13</th>\n",
       "      <th>fea_14</th>\n",
       "      <th>fea_15</th>\n",
       "      <th>fea_16</th>\n",
       "      <th>fea_17</th>\n",
       "      <th>fea_18</th>\n",
       "      <th>fea_19</th>\n",
       "      <th>fea_20</th>\n",
       "      <th>fea_21</th>\n",
       "      <th>fea_22</th>\n",
       "      <th>fea_23</th>\n",
       "      <th>fea_24</th>\n",
       "      <th>fea_25</th>\n",
       "      <th>fea_26</th>\n",
       "      <th>fea_27</th>\n",
       "      <th>fea_28</th>\n",
       "      <th>fea_29</th>\n",
       "      <th>fea_30</th>\n",
       "      <th>fea_31</th>\n",
       "      <th>fea_32</th>\n",
       "      <th>fea_33</th>\n",
       "      <th>fea_34</th>\n",
       "      <th>fea_35</th>\n",
       "      <th>fea_36</th>\n",
       "      <th>fea_37</th>\n",
       "      <th>fea_38</th>\n",
       "      <th>...</th>\n",
       "      <th>fea_216</th>\n",
       "      <th>fea_217</th>\n",
       "      <th>fea_218</th>\n",
       "      <th>fea_219</th>\n",
       "      <th>fea_220</th>\n",
       "      <th>fea_221</th>\n",
       "      <th>fea_222</th>\n",
       "      <th>fea_223</th>\n",
       "      <th>fea_224</th>\n",
       "      <th>fea_225</th>\n",
       "      <th>fea_226</th>\n",
       "      <th>fea_227</th>\n",
       "      <th>fea_228</th>\n",
       "      <th>fea_229</th>\n",
       "      <th>fea_230</th>\n",
       "      <th>fea_231</th>\n",
       "      <th>fea_232</th>\n",
       "      <th>fea_233</th>\n",
       "      <th>fea_234</th>\n",
       "      <th>fea_235</th>\n",
       "      <th>fea_236</th>\n",
       "      <th>fea_237</th>\n",
       "      <th>fea_238</th>\n",
       "      <th>fea_239</th>\n",
       "      <th>fea_240</th>\n",
       "      <th>fea_241</th>\n",
       "      <th>fea_242</th>\n",
       "      <th>fea_243</th>\n",
       "      <th>fea_244</th>\n",
       "      <th>fea_245</th>\n",
       "      <th>fea_246</th>\n",
       "      <th>fea_247</th>\n",
       "      <th>fea_248</th>\n",
       "      <th>fea_249</th>\n",
       "      <th>fea_250</th>\n",
       "      <th>fea_251</th>\n",
       "      <th>fea_252</th>\n",
       "      <th>fea_253</th>\n",
       "      <th>fea_254</th>\n",
       "      <th>fea_255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.360416</td>\n",
       "      <td>-0.683295</td>\n",
       "      <td>-0.659030</td>\n",
       "      <td>-1.840172</td>\n",
       "      <td>-1.875868</td>\n",
       "      <td>0.369852</td>\n",
       "      <td>-0.806902</td>\n",
       "      <td>-0.210944</td>\n",
       "      <td>-1.371679</td>\n",
       "      <td>-1.099853</td>\n",
       "      <td>-1.660540</td>\n",
       "      <td>-2.700233</td>\n",
       "      <td>-1.127029</td>\n",
       "      <td>-1.378679</td>\n",
       "      <td>-0.307012</td>\n",
       "      <td>-1.190641</td>\n",
       "      <td>-1.230518</td>\n",
       "      <td>-1.370454</td>\n",
       "      <td>-1.691368</td>\n",
       "      <td>-1.579029</td>\n",
       "      <td>0.082165</td>\n",
       "      <td>-1.161934</td>\n",
       "      <td>-1.383861</td>\n",
       "      <td>-0.755309</td>\n",
       "      <td>-1.822869</td>\n",
       "      <td>-0.869281</td>\n",
       "      <td>-1.414214</td>\n",
       "      <td>-1.299370</td>\n",
       "      <td>-1.005880</td>\n",
       "      <td>-1.500478</td>\n",
       "      <td>-1.513924</td>\n",
       "      <td>-1.133812</td>\n",
       "      <td>-2.309000</td>\n",
       "      <td>-2.411295</td>\n",
       "      <td>-1.763362</td>\n",
       "      <td>-1.367103</td>\n",
       "      <td>-1.316206</td>\n",
       "      <td>-1.253621</td>\n",
       "      <td>-1.061936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443950</td>\n",
       "      <td>-0.240541</td>\n",
       "      <td>-0.458982</td>\n",
       "      <td>-1.973151</td>\n",
       "      <td>-2.266466</td>\n",
       "      <td>-0.900079</td>\n",
       "      <td>-1.011891</td>\n",
       "      <td>-1.575932</td>\n",
       "      <td>-0.577731</td>\n",
       "      <td>-0.882220</td>\n",
       "      <td>-1.955588</td>\n",
       "      <td>-1.812867</td>\n",
       "      <td>-0.655008</td>\n",
       "      <td>-0.674895</td>\n",
       "      <td>-0.402153</td>\n",
       "      <td>-1.214736</td>\n",
       "      <td>-0.068207</td>\n",
       "      <td>-0.507292</td>\n",
       "      <td>-1.012512</td>\n",
       "      <td>-0.988932</td>\n",
       "      <td>-1.453708</td>\n",
       "      <td>-0.972928</td>\n",
       "      <td>-1.479248</td>\n",
       "      <td>-1.644744</td>\n",
       "      <td>-1.165418</td>\n",
       "      <td>-1.138936</td>\n",
       "      <td>-0.723940</td>\n",
       "      <td>-2.013051</td>\n",
       "      <td>-2.337651</td>\n",
       "      <td>-0.875384</td>\n",
       "      <td>-1.287771</td>\n",
       "      <td>-0.003345</td>\n",
       "      <td>-1.017979</td>\n",
       "      <td>-0.849153</td>\n",
       "      <td>-1.564582</td>\n",
       "      <td>-1.167882</td>\n",
       "      <td>-1.138140</td>\n",
       "      <td>-0.459417</td>\n",
       "      <td>-1.342706</td>\n",
       "      <td>-0.491899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-5.414928</td>\n",
       "      <td>-4.034106</td>\n",
       "      <td>-6.748904</td>\n",
       "      <td>-4.867840</td>\n",
       "      <td>-8.706087</td>\n",
       "      <td>-8.195160</td>\n",
       "      <td>-5.521785</td>\n",
       "      <td>-5.785634</td>\n",
       "      <td>-6.909437</td>\n",
       "      <td>-9.894236</td>\n",
       "      <td>-6.763881</td>\n",
       "      <td>-6.437338</td>\n",
       "      <td>-7.996785</td>\n",
       "      <td>-8.144830</td>\n",
       "      <td>-5.853707</td>\n",
       "      <td>-8.095213</td>\n",
       "      <td>-8.178341</td>\n",
       "      <td>-6.787964</td>\n",
       "      <td>-9.715798</td>\n",
       "      <td>-7.277096</td>\n",
       "      <td>-9.050832</td>\n",
       "      <td>-10.176917</td>\n",
       "      <td>-6.833709</td>\n",
       "      <td>-5.547260</td>\n",
       "      <td>-6.475846</td>\n",
       "      <td>-1.981348</td>\n",
       "      <td>-9.246424</td>\n",
       "      <td>-10.029983</td>\n",
       "      <td>-9.742969</td>\n",
       "      <td>-9.796604</td>\n",
       "      <td>-6.105887</td>\n",
       "      <td>-7.081398</td>\n",
       "      <td>-4.782876</td>\n",
       "      <td>-7.618280</td>\n",
       "      <td>-6.487199</td>\n",
       "      <td>-7.361187</td>\n",
       "      <td>-3.467677</td>\n",
       "      <td>-11.332512</td>\n",
       "      <td>-6.311137</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.110875</td>\n",
       "      <td>-6.635378</td>\n",
       "      <td>-6.384874</td>\n",
       "      <td>-8.538627</td>\n",
       "      <td>-7.053156</td>\n",
       "      <td>-7.058464</td>\n",
       "      <td>-9.186590</td>\n",
       "      <td>-8.764717</td>\n",
       "      <td>-6.414261</td>\n",
       "      <td>-6.514872</td>\n",
       "      <td>-4.659565</td>\n",
       "      <td>-5.857071</td>\n",
       "      <td>-8.454607</td>\n",
       "      <td>-3.282822</td>\n",
       "      <td>-9.523328</td>\n",
       "      <td>-5.563709</td>\n",
       "      <td>-4.822659</td>\n",
       "      <td>-8.675863</td>\n",
       "      <td>-7.214526</td>\n",
       "      <td>-10.050051</td>\n",
       "      <td>-5.550786</td>\n",
       "      <td>-4.974769</td>\n",
       "      <td>-6.917077</td>\n",
       "      <td>-8.980473</td>\n",
       "      <td>-5.929104</td>\n",
       "      <td>-6.532549</td>\n",
       "      <td>-2.053741</td>\n",
       "      <td>-5.098194</td>\n",
       "      <td>-9.574401</td>\n",
       "      <td>-6.833381</td>\n",
       "      <td>-6.531560</td>\n",
       "      <td>-8.419637</td>\n",
       "      <td>-9.145112</td>\n",
       "      <td>-4.200750</td>\n",
       "      <td>-7.780539</td>\n",
       "      <td>-4.257525</td>\n",
       "      <td>-5.879356</td>\n",
       "      <td>-8.006350</td>\n",
       "      <td>-9.809999</td>\n",
       "      <td>-8.942007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.742383</td>\n",
       "      <td>-0.772285</td>\n",
       "      <td>-0.565367</td>\n",
       "      <td>-1.349759</td>\n",
       "      <td>-0.456425</td>\n",
       "      <td>-0.973115</td>\n",
       "      <td>-1.273366</td>\n",
       "      <td>-0.878384</td>\n",
       "      <td>-0.554383</td>\n",
       "      <td>-1.510590</td>\n",
       "      <td>-0.781685</td>\n",
       "      <td>-1.537488</td>\n",
       "      <td>-1.844128</td>\n",
       "      <td>-1.762580</td>\n",
       "      <td>-0.678578</td>\n",
       "      <td>-0.737251</td>\n",
       "      <td>-0.709060</td>\n",
       "      <td>-1.957293</td>\n",
       "      <td>-0.955822</td>\n",
       "      <td>-1.370779</td>\n",
       "      <td>-1.176953</td>\n",
       "      <td>-0.794090</td>\n",
       "      <td>-0.990380</td>\n",
       "      <td>-0.722812</td>\n",
       "      <td>-1.586224</td>\n",
       "      <td>0.022261</td>\n",
       "      <td>-0.557890</td>\n",
       "      <td>-0.664564</td>\n",
       "      <td>-0.589544</td>\n",
       "      <td>-1.534760</td>\n",
       "      <td>-1.432168</td>\n",
       "      <td>-0.604248</td>\n",
       "      <td>-1.251627</td>\n",
       "      <td>-1.823051</td>\n",
       "      <td>-0.750866</td>\n",
       "      <td>-0.249489</td>\n",
       "      <td>-1.184111</td>\n",
       "      <td>-1.566427</td>\n",
       "      <td>-1.756267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.880831</td>\n",
       "      <td>-0.881381</td>\n",
       "      <td>-0.910604</td>\n",
       "      <td>-0.595922</td>\n",
       "      <td>-1.336740</td>\n",
       "      <td>-1.226703</td>\n",
       "      <td>-0.650778</td>\n",
       "      <td>-0.507728</td>\n",
       "      <td>-1.110144</td>\n",
       "      <td>-0.545716</td>\n",
       "      <td>-0.837331</td>\n",
       "      <td>-1.571738</td>\n",
       "      <td>-1.712780</td>\n",
       "      <td>-0.440420</td>\n",
       "      <td>-0.716094</td>\n",
       "      <td>-0.852922</td>\n",
       "      <td>-1.398606</td>\n",
       "      <td>-1.358359</td>\n",
       "      <td>-1.410937</td>\n",
       "      <td>-0.343936</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>-0.724121</td>\n",
       "      <td>-0.792205</td>\n",
       "      <td>-0.497975</td>\n",
       "      <td>-0.768428</td>\n",
       "      <td>-0.796491</td>\n",
       "      <td>-0.561063</td>\n",
       "      <td>-1.110194</td>\n",
       "      <td>-1.327270</td>\n",
       "      <td>-0.814120</td>\n",
       "      <td>-1.065703</td>\n",
       "      <td>-0.614035</td>\n",
       "      <td>-0.517165</td>\n",
       "      <td>-0.655323</td>\n",
       "      <td>-1.019581</td>\n",
       "      <td>-1.706855</td>\n",
       "      <td>-1.084116</td>\n",
       "      <td>-0.940970</td>\n",
       "      <td>-0.905574</td>\n",
       "      <td>-1.151560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.853681</td>\n",
       "      <td>-1.021450</td>\n",
       "      <td>-0.639012</td>\n",
       "      <td>-2.095687</td>\n",
       "      <td>-1.756671</td>\n",
       "      <td>-0.450270</td>\n",
       "      <td>-0.607497</td>\n",
       "      <td>-0.755120</td>\n",
       "      <td>-0.827463</td>\n",
       "      <td>-1.325550</td>\n",
       "      <td>-1.401466</td>\n",
       "      <td>-2.182161</td>\n",
       "      <td>-1.297098</td>\n",
       "      <td>-1.523485</td>\n",
       "      <td>-1.380138</td>\n",
       "      <td>-0.635918</td>\n",
       "      <td>-0.961911</td>\n",
       "      <td>-2.159257</td>\n",
       "      <td>-1.128361</td>\n",
       "      <td>-0.837077</td>\n",
       "      <td>-0.103479</td>\n",
       "      <td>-1.213436</td>\n",
       "      <td>-1.127160</td>\n",
       "      <td>-1.567182</td>\n",
       "      <td>-1.756458</td>\n",
       "      <td>-1.276088</td>\n",
       "      <td>-1.111948</td>\n",
       "      <td>-1.226602</td>\n",
       "      <td>-1.184764</td>\n",
       "      <td>-0.981543</td>\n",
       "      <td>-1.224701</td>\n",
       "      <td>-1.663145</td>\n",
       "      <td>-1.035740</td>\n",
       "      <td>-2.551293</td>\n",
       "      <td>-1.625138</td>\n",
       "      <td>-0.755506</td>\n",
       "      <td>-1.310180</td>\n",
       "      <td>-1.248310</td>\n",
       "      <td>-1.491312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523887</td>\n",
       "      <td>-0.638467</td>\n",
       "      <td>-1.005439</td>\n",
       "      <td>-1.053905</td>\n",
       "      <td>-1.289685</td>\n",
       "      <td>-1.115545</td>\n",
       "      <td>-0.768461</td>\n",
       "      <td>-1.078914</td>\n",
       "      <td>-0.270053</td>\n",
       "      <td>-0.839568</td>\n",
       "      <td>-1.904441</td>\n",
       "      <td>-0.477862</td>\n",
       "      <td>-1.423661</td>\n",
       "      <td>-1.016707</td>\n",
       "      <td>-0.541361</td>\n",
       "      <td>-1.327548</td>\n",
       "      <td>-0.263632</td>\n",
       "      <td>-0.397027</td>\n",
       "      <td>-0.754340</td>\n",
       "      <td>-1.027716</td>\n",
       "      <td>-0.743639</td>\n",
       "      <td>-0.632008</td>\n",
       "      <td>-1.050929</td>\n",
       "      <td>-1.753120</td>\n",
       "      <td>-1.868257</td>\n",
       "      <td>-0.667384</td>\n",
       "      <td>-0.857556</td>\n",
       "      <td>-2.483495</td>\n",
       "      <td>-2.112216</td>\n",
       "      <td>-0.667267</td>\n",
       "      <td>-1.223609</td>\n",
       "      <td>-1.181383</td>\n",
       "      <td>-0.928340</td>\n",
       "      <td>-0.841091</td>\n",
       "      <td>-1.477898</td>\n",
       "      <td>-1.802510</td>\n",
       "      <td>-1.177191</td>\n",
       "      <td>-1.632113</td>\n",
       "      <td>-1.980912</td>\n",
       "      <td>-1.298478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.049173</td>\n",
       "      <td>-0.208819</td>\n",
       "      <td>-1.020380</td>\n",
       "      <td>-1.916308</td>\n",
       "      <td>-1.213041</td>\n",
       "      <td>0.404414</td>\n",
       "      <td>-1.085374</td>\n",
       "      <td>-1.219756</td>\n",
       "      <td>-0.854078</td>\n",
       "      <td>-1.226236</td>\n",
       "      <td>-0.990248</td>\n",
       "      <td>-1.566594</td>\n",
       "      <td>-0.792700</td>\n",
       "      <td>-1.217160</td>\n",
       "      <td>-0.948019</td>\n",
       "      <td>-1.083676</td>\n",
       "      <td>0.248386</td>\n",
       "      <td>-0.625715</td>\n",
       "      <td>-0.667505</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>-0.728039</td>\n",
       "      <td>-1.420544</td>\n",
       "      <td>-1.330699</td>\n",
       "      <td>-1.037147</td>\n",
       "      <td>-0.644503</td>\n",
       "      <td>-0.775842</td>\n",
       "      <td>-1.119213</td>\n",
       "      <td>-0.565874</td>\n",
       "      <td>-1.670511</td>\n",
       "      <td>-2.027897</td>\n",
       "      <td>-1.344954</td>\n",
       "      <td>-1.358789</td>\n",
       "      <td>-1.530087</td>\n",
       "      <td>-1.639055</td>\n",
       "      <td>-1.687073</td>\n",
       "      <td>-1.852848</td>\n",
       "      <td>-1.298267</td>\n",
       "      <td>-1.585940</td>\n",
       "      <td>-1.196822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195807</td>\n",
       "      <td>-0.552347</td>\n",
       "      <td>-0.815995</td>\n",
       "      <td>-2.064569</td>\n",
       "      <td>-1.841970</td>\n",
       "      <td>-0.262925</td>\n",
       "      <td>-0.424365</td>\n",
       "      <td>-1.394225</td>\n",
       "      <td>-0.073432</td>\n",
       "      <td>-0.594325</td>\n",
       "      <td>-1.921384</td>\n",
       "      <td>-1.289113</td>\n",
       "      <td>-1.390133</td>\n",
       "      <td>-0.598373</td>\n",
       "      <td>-0.319866</td>\n",
       "      <td>-0.918305</td>\n",
       "      <td>0.144083</td>\n",
       "      <td>-0.586340</td>\n",
       "      <td>-0.346057</td>\n",
       "      <td>-0.666028</td>\n",
       "      <td>-0.949032</td>\n",
       "      <td>-0.438806</td>\n",
       "      <td>-1.266952</td>\n",
       "      <td>-1.054088</td>\n",
       "      <td>-1.777013</td>\n",
       "      <td>-0.808951</td>\n",
       "      <td>-0.275256</td>\n",
       "      <td>-1.907677</td>\n",
       "      <td>-2.523139</td>\n",
       "      <td>-1.578980</td>\n",
       "      <td>-1.427951</td>\n",
       "      <td>-0.035228</td>\n",
       "      <td>-0.856884</td>\n",
       "      <td>-1.142188</td>\n",
       "      <td>-1.742321</td>\n",
       "      <td>-0.767455</td>\n",
       "      <td>0.114727</td>\n",
       "      <td>-0.083121</td>\n",
       "      <td>-1.886625</td>\n",
       "      <td>-0.243228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     fea_0     fea_1  ...   fea_253   fea_254   fea_255\n",
       "0           0 -1.360416 -0.683295  ... -0.459417 -1.342706 -0.491899\n",
       "1           1 -5.414928 -4.034106  ... -8.006350 -9.809999 -8.942007\n",
       "2           2 -0.742383 -0.772285  ... -0.940970 -0.905574 -1.151560\n",
       "3           3 -0.853681 -1.021450  ... -1.632113 -1.980912 -1.298478\n",
       "4           4 -1.049173 -0.208819  ... -0.083121 -1.886625 -0.243228\n",
       "\n",
       "[5 rows x 257 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df = pd.read_csv('/content/drive/My Drive/res2021/flickr_item_fea.csv')\n",
    "items_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hztMPU8T6FaJ"
   },
   "source": [
    "Same thing than with users, but with items.\n",
    "\n",
    "### Social links data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "cIeMfjDMBJ42",
    "outputId": "2ce29ca5-ea76-4ef9-9b74-389faedd4bb0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>des</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1431</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>592</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   src   des  weight\n",
       "0    0  1431       1\n",
       "1    0   955       1\n",
       "2    0  1824       1\n",
       "3    0    70       1\n",
       "4    0   592       1"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_df = pd.read_csv('/content/drive/My Drive/res2021/flickr_links.csv')\n",
    "links_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkzaL0l06M3E"
   },
   "source": [
    "This is actually binary data stating if 2 people are friends or not, it's not pre-trained data like the previous 2. We will try to use it as well, but will have to transform it first into its sparsed version and then factorize it.\n",
    "\n",
    "### General checkings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIUDLSiiNlBA"
   },
   "source": [
    "Let's also check how many different users and items we have among our training data, and also if we don't have any user missing (meaning all our users are consecutive, which will be important afterwards while building the different matrices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RE6QUWPcHd8B",
    "outputId": "02a9a35f-b82e-4523-efeb-9b5d21cf5f19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All user_ids are consecutive and there are 3466\n"
     ]
    }
   ],
   "source": [
    "# variable that will be activated if 2 user_ids are not consecutives\n",
    "not_consecutive = 0\n",
    "# unique users\n",
    "unique_users = train_data.user_id.unique()\n",
    "# number of different users\n",
    "n_users = len(unique_users)\n",
    "# looping over all the users\n",
    "for i in unique_users:\n",
    "    # if 2 consecutive users have not consecutive ids, activate not_consecutive\n",
    "    # and print the index\n",
    "    if unique_users[i] != unique_users[i-1] + 1:\n",
    "        if i != 0:\n",
    "            print(i)\n",
    "            not_consecutive = 1\n",
    "if not_consecutive == 0:\n",
    "  print('All user_ids are consecutive and there are', n_users)\n",
    "  n_items = len(train_data.item_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0K2g7VOWJNet",
    "outputId": "a6996136-66f6-4443-d40d-db321cc0fce0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9004"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_items = len(train_data.item_id.unique())\n",
    "n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHGEZPIwO3nn"
   },
   "source": [
    "We can see that we have 3466 distinct users and 9004 different images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLokUM0HPARp"
   },
   "source": [
    "## III. Building the models\n",
    "\n",
    "### MF, GMF and MLP basic models\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Now I will proceed to building the different Models, using the Neural Networks approach for all of them. Firstly we will present the very basic version of the different alternatives, in order to understand which will be the one that could potentially perform better. The basic alternatives that we are firstly going to assess are the following:\n",
    "\n",
    "* Matrix Factorization (MF) Model that has an embedding for the users and the items and we make the prediction using a product between these 2.\n",
    "* Generalized Matrix Factorization (GMF) Model in which we add an extra layer to model the interaction between user and item.\n",
    "* Multi-Layer Perceptron (MLP) that consists of three types of layers: input, output and hidden layers, that can be useful to model any non-linear relationship between users and items.\n",
    "\n",
    "#### Adding negative samples to the data\n",
    "\n",
    "But before diving into building the models, we have to remark that the training data only has positive instances, meaning that there is only data about users and items that have interacted. There is not data about items that users have not interacted with. And it's important to include negative samples among our training data, so that our model can really learn what the user likes and rank it higher for instance than items he dislikes or has not interacted with. To try our basic models, we will take 5 negative samples (items the user has not interacted with) per every item the user has interacted with. Note that this N (in this case 5) becomes a hyperparameter that we will be able to tune afterwards when we have decided which model to continue with. Adding the negative samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejcf-ToVWtnw"
   },
   "outputs": [],
   "source": [
    "# defining a function to be able to reuse it\n",
    "def negative_sampler(neg_factor):\n",
    "    # number_of_negative_samples = neg_factor * number_of_items_a_user_has_interacted_with\n",
    "    # initializing data frame that will end up containing the training data +\n",
    "    # the negative samples\n",
    "    negative_samples_df = pd.DataFrame()\n",
    "    # all the different items contained in training set\n",
    "    all_items = set(train_data.item_id)\n",
    "    # looping over all the different users\n",
    "    for user_id in train_data.user_id.unique():\n",
    "        # subsetting with user_id\n",
    "        train_data_subset = train_data[train_data.user_id==user_id]\n",
    "        # getting the number of different items the particular user has\n",
    "        # interacted with\n",
    "        quant_items_interaction = len(train_data_subset.item_id.unique())\n",
    "        # items the user has not interacted with\n",
    "        items_no_interaction = all_items - set(train_data_subset.item_id)\n",
    "        # we will take n times the number of items the user has interacted with \n",
    "        # as negative samples and if n*items exceeds the number of remaining items \n",
    "        # we will be in trouble, so if this happens we just take all the items\n",
    "        # the user has not interacted with\n",
    "        if quant_items_interaction * neg_factor > len(items_no_interaction):\n",
    "            sample_size = len(items_no_interaction)\n",
    "        else:\n",
    "            sample_size = neg_factor * quant_items_interaction\n",
    "        # taking a random sample from the items the user has not interacted with\n",
    "        items_no_interaction_sample = random.sample(items_no_interaction, sample_size)\n",
    "        # initializing data frame that will be concatenated to the general data frame\n",
    "        negative_samples = pd.DataFrame({'user_id':[user_id]*len(items_no_interaction_sample), 'item_id':items_no_interaction_sample, 'rating':[0]*len(items_no_interaction_sample)})\n",
    "        # concatenating the existing positive cases and the sampled negative ones for each user\n",
    "        train_data_subset = pd.concat([train_data_subset, negative_samples], ignore_index = True)\n",
    "        # concatenating with the whole data\n",
    "        negative_samples_df = pd.concat([negative_samples_df, train_data_subset], ignore_index = True)\n",
    "    # ordering by user and then by item\n",
    "    negative_samples_df = negative_samples_df.sort_values(by = ['user_id', 'item_id'])\n",
    "    return(negative_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPM6y0rSaJ6H"
   },
   "outputs": [],
   "source": [
    "# calling the function with factor of 5\n",
    "negative_samples_df = negative_sampler(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "LPCMIt5SarSx",
    "outputId": "b90d1563-a777-43d6-f681-8cbdd889d2c8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>8679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>8708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>8795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>8884</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>8960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id  rating\n",
       "0         0        0       1\n",
       "1         0        1       1\n",
       "2         0        2       1\n",
       "3         0        3       1\n",
       "4         0        4       1\n",
       "..      ...      ...     ...\n",
       "89        0     8679       0\n",
       "82        0     8708       0\n",
       "69        0     8795       0\n",
       "23        0     8884       0\n",
       "33        0     8960       0\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see how now for example user 0 has both items he has interacted with and\n",
    "# items he hasn't\n",
    "negative_samples_df[negative_samples_df.user_id==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GP4_MVf3bGnE",
    "outputId": "495b046e-7ab9-44c1-ee7f-5e2bd4896c24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 20 positive samples and 100 negative samples\n"
     ]
    }
   ],
   "source": [
    "negative_samples = len(negative_samples_df[(negative_samples_df.user_id==0) & (negative_samples_df.rating==0)])\n",
    "positive_samples = len(negative_samples_df[(negative_samples_df.user_id==0) & (negative_samples_df.rating==1)])\n",
    "print('We have', positive_samples, 'positive samples and', negative_samples, 'negative samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpkMvlyCb5uD"
   },
   "source": [
    "We can see that we have 5 times more of negative samples than positive's. We could do this to capture the complete sparsity of the matrix, but it would take a long time to train all our models, so this will do it for the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzOlRg9vnWzN"
   },
   "source": [
    "### The models\n",
    "\n",
    "We are going now to define the classes for the 3 different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IFVrV6OdHNt"
   },
   "outputs": [],
   "source": [
    "# defining the basic Matrix Factorization model\n",
    "class MF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=256):\n",
    "        super(MF, self).__init__()\n",
    "        # embedding corresponding to the user matrix\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        # embedding corresponding to the item matrix\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        # initialization of the values of the matrices\n",
    "        self.user_emb.weight.data.uniform_(0, 0.05)\n",
    "        self.item_emb.weight.data.uniform_(0, 0.05)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        # the updated value for the matrices for each user/item\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        # product between the matrices to return the prediction\n",
    "        return (U*V).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJ8In9rjdHve"
   },
   "outputs": [],
   "source": [
    "# defining the Generalized Matrix Factorization model \n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, n_user, n_item, n_emb=8):\n",
    "        super(GMF, self).__init__()\n",
    "\n",
    "        self.n_emb = n_emb\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        # similar to the previous one but we add an extra layer to model\n",
    "        # the interaction between user and item\n",
    "        self.embeddings_user = nn.Embedding(n_user, n_emb)\n",
    "        self.embeddings_item = nn.Embedding(n_item, n_emb)\n",
    "        self.out = nn.Linear(in_features=n_emb, out_features=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight)\n",
    "\n",
    "    def forward(self, users, items):\n",
    "\n",
    "        user_emb = self.embeddings_user(users)\n",
    "        item_emb = self.embeddings_item(items)\n",
    "        # multiplication between matrices\n",
    "        prod = user_emb*item_emb\n",
    "        # activation of the multiplication will give the final predictions\n",
    "        preds = torch.sigmoid(self.out(prod))\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWKbNBshdKnv"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_user, n_item, layers, dropouts):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.dropouts = dropouts\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        # initializing users and items embeddings as in the previous models\n",
    "        self.embeddings_user = nn.Embedding(n_user, int(layers[0]/2))\n",
    "        self.embeddings_item = nn.Embedding(n_item, int(layers[0]/2))\n",
    "        # defining the hidden layers\n",
    "        self.mlp = nn.Sequential()\n",
    "        for i in range(1,self.n_layers):\n",
    "            self.mlp.add_module(\"linear%d\" %i, nn.Linear(layers[i-1],layers[i]))\n",
    "            self.mlp.add_module(\"relu%d\" %i, torch.nn.ReLU())\n",
    "            self.mlp.add_module(\"dropout%d\" %i , torch.nn.Dropout(p=dropouts[i-1]))\n",
    "        # output layer\n",
    "        self.out = nn.Linear(in_features=layers[-1], out_features=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight)\n",
    "\n",
    "    def forward(self, users, items):\n",
    "\n",
    "        user_emb = self.embeddings_user(users)\n",
    "        item_emb = self.embeddings_item(items)\n",
    "        # different than the other 2 previous models, we don't perform product\n",
    "        # between the matrices but we concatenate them\n",
    "        emb_vector = torch.cat([user_emb,item_emb], dim=1)\n",
    "        emb_vector = self.mlp(emb_vector)\n",
    "        preds = torch.sigmoid(self.out(emb_vector))\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uhdbots-tFu0"
   },
   "source": [
    "Once we have defined our model classes, we can also define a function to train these models and another one to evaluate them.\n",
    "\n",
    "First, the training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etNQ4Wu8rcLf"
   },
   "outputs": [],
   "source": [
    "def train_epocs(model_name , train_data, epochs=10, lr=0.01, wd=0.0, unsqueeze=False):\n",
    "    # defining optimizer that will update the weights of our model trying to minimize\n",
    "    # loss function\n",
    "    print('MODEL:' , model_name, 'LEARNING RATE =', lr)\n",
    "    model = model_dict[model_name][0]\n",
    "    criterion = model_dict[model_name][1]\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        # transforming input to tensor\n",
    "        users = torch.LongTensor(train_data.user_id.values).cuda()\n",
    "        items = torch.LongTensor(train_data.item_id.values).cuda()\n",
    "        ratings = torch.FloatTensor(train_data.rating.values).cuda()\n",
    "        if unsqueeze:\n",
    "            ratings = ratings.unsqueeze(1)\n",
    "        if model_name == 'GMF' or model_name == 'MLP':\n",
    "          preds = model(users, items).squeeze(1)\n",
    "        else:\n",
    "          preds = model(users, items)\n",
    "        # defining loss function\n",
    "        loss = criterion(preds, ratings)\n",
    "        # to update the parameters when evaluating\n",
    "        optimizer.zero_grad()\n",
    "        # calculating gradient\n",
    "        loss.backward()\n",
    "        # updating parameters\n",
    "        optimizer.step()\n",
    "        print('EPOC', i, 'LOSS:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwoQziT9uDYA"
   },
   "source": [
    "Then, all the functions that relate to the evaluation of the performances of our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsfraAozEUXg"
   },
   "outputs": [],
   "source": [
    "# defining the metric we are going to use when evaluating the different models\n",
    "# NDCG is a metric that cares about which proportion of recommended items\n",
    "# is accurate as well as their order of importance\n",
    "def get_ndcg(ranklist, gtitem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtitem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QX4D5PJHtimK"
   },
   "outputs": [],
   "source": [
    "# defining function that will use the above to retrieve the performance metric\n",
    "def get_scores(items, preds, topk):\n",
    "    gtitem = items[0]\n",
    "    # the following 3 lines of code ensure that the fact that the 1st item is\n",
    "    # gtitem does not affect the final rank\n",
    "    randidx = np.arange(100)\n",
    "    np.random.shuffle(randidx)\n",
    "    items, preds = items[randidx], preds[randidx]\n",
    "    map_item_score = dict( zip(items, preds) )\n",
    "    # getting the topk user-item pairs with the highest scores (ordered)\n",
    "    ranklist = heapq.nlargest(topk, map_item_score, key=map_item_score.get)\n",
    "    ndcg = get_ndcg(ranklist, gtitem)\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6W3VTud62yy8"
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model_name, val_loader, use_cuda, topk):\n",
    "    # with model.eval() our parameters will not be updated\n",
    "    model = model_dict[model_name][0]\n",
    "    model.eval()\n",
    "    scores=[]\n",
    "    with torch.no_grad():\n",
    "        # looping each batch in validation loader\n",
    "        for data in val_loader:\n",
    "            users = data[0]\n",
    "            items = data[1]\n",
    "            labels = data[2].float()\n",
    "            if use_cuda:\n",
    "                users, items, labels = users.cuda(), items.cuda(), labels.cuda()\n",
    "            # the predictions\n",
    "            preds = model(users, items)\n",
    "            items_cpu = items.cpu().numpy()\n",
    "            preds_cpu = preds.detach().cpu().numpy()\n",
    "            litems=np.split(items_cpu, val_loader.batch_size//100)\n",
    "            lpreds=np.split(preds_cpu, val_loader.batch_size//100)\n",
    "            scores += [get_scores(it,pr,topk) for it,pr in zip(litems,lpreds)]\n",
    "    ndcg = np.array(scores).mean()\n",
    "    print(model_name, 'validation NDCG:', ndcg)\n",
    "    print('\\n')\n",
    "    return (ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Lw3vU3bFa6J"
   },
   "outputs": [],
   "source": [
    "# defining the validation loader that is used in the above function.\n",
    "# the size is 100 because we are given (in validation and test), 100 possible\n",
    "# items to choose from for every user.\n",
    "\n",
    "users_tensor_val = torch.tensor(val_data.user_id.tolist())\n",
    "items_tensor_val = torch.tensor(val_data.item_id.tolist())\n",
    "target_tensor_val = torch.tensor(val_data.rating.tolist())\n",
    "dataval = TensorDataset(users_tensor_val, items_tensor_val, target_tensor_val)\n",
    "\n",
    "val_loader = DataLoader(dataset=dataval,\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0ujtQnCu6w7"
   },
   "source": [
    "### Training the models\n",
    "We proceed to initializing each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wn75c4IFGAoN"
   },
   "outputs": [],
   "source": [
    "model_GMF = GMF(n_users, n_items, 256)\n",
    "criterion_GMF = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afDBux0Yiy9u"
   },
   "outputs": [],
   "source": [
    "model_MF = MF(n_users, n_items, 256)\n",
    "criterion_MF = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSMWj7V2i2dC"
   },
   "outputs": [],
   "source": [
    "layers = [128, 64, 32, 16] # first layer is n_emb*2\n",
    "dropouts = [0., 0., 0.0] # len(dropouts) = len(layers)-1\n",
    "model_MLP = MLP(n_users, n_items, layers, dropouts)\n",
    "criterion_MLP = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsvnLokUxWk9"
   },
   "outputs": [],
   "source": [
    "# we store our models and optimisers in a dictionary for easier manipulation\n",
    "model_dict = dict()\n",
    "model_dict['GMF'] = [model_GMF, criterion_GMF]\n",
    "model_dict['MF'] = [model_MF, criterion_MF]\n",
    "model_dict['MLP'] = [model_MLP, criterion_MLP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1kSlHkTHbEj"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model_dict['GMF'] = [model_dict['GMF'][0].cuda(), model_dict['GMF'][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nZHdvD5j7-c"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model_dict['MF'] = [model_dict['MF'][0].cuda(), model_dict['MF'][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqKxuuy_j8Ha"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model_dict['MLP'] = [model_dict['MLP'][0].cuda(), model_dict['MLP'][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhsFASZlvMdj"
   },
   "source": [
    "And now we are going to train each of the models and then evaluate them using the validation test, to see which one we are going to use for further analysis and tunning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N18vx_uYvazb",
    "outputId": "f9027966-b5b8-4ad2-dd16-7c9f38bf89fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: GMF LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 6.551529884338379\n",
      "EPOC 1 LOSS: 3.2693233489990234\n",
      "EPOC 2 LOSS: 1.5473445653915405\n",
      "EPOC 3 LOSS: 0.800610363483429\n",
      "EPOC 4 LOSS: 0.4851033687591553\n",
      "EPOC 5 LOSS: 0.3324180245399475\n",
      "EPOC 6 LOSS: 0.25514301657676697\n",
      "EPOC 7 LOSS: 0.21580785512924194\n",
      "EPOC 8 LOSS: 0.1966424137353897\n",
      "EPOC 9 LOSS: 0.1866900473833084\n",
      "EPOC 10 LOSS: 0.17668569087982178\n",
      "EPOC 11 LOSS: 0.1606050729751587\n",
      "EPOC 12 LOSS: 0.13842368125915527\n",
      "EPOC 13 LOSS: 0.11340328305959702\n",
      "EPOC 14 LOSS: 0.08827237039804459\n",
      "EPOC 15 LOSS: 0.06483779102563858\n",
      "EPOC 16 LOSS: 0.044650908559560776\n",
      "EPOC 17 LOSS: 0.028863536193966866\n",
      "EPOC 18 LOSS: 0.017734767869114876\n",
      "EPOC 19 LOSS: 0.010593939572572708\n",
      "GMF validation NDCG: 0.06168397036257753\n",
      "\n",
      "\n",
      "MODEL: MF LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 0.749582827091217\n",
      "EPOC 1 LOSS: 1.3975855112075806\n",
      "EPOC 2 LOSS: 0.7108043432235718\n",
      "EPOC 3 LOSS: 0.8337104916572571\n",
      "EPOC 4 LOSS: 1.1111959218978882\n",
      "EPOC 5 LOSS: 0.9124086499214172\n",
      "EPOC 6 LOSS: 0.7134793400764465\n",
      "EPOC 7 LOSS: 0.6938778162002563\n",
      "EPOC 8 LOSS: 0.7866360545158386\n",
      "EPOC 9 LOSS: 0.8432669639587402\n",
      "EPOC 10 LOSS: 0.7401682734489441\n",
      "EPOC 11 LOSS: 0.5676214694976807\n",
      "EPOC 12 LOSS: 0.44236692786216736\n",
      "EPOC 13 LOSS: 0.41608157753944397\n",
      "EPOC 14 LOSS: 0.4552854597568512\n",
      "EPOC 15 LOSS: 0.47205957770347595\n",
      "EPOC 16 LOSS: 0.43055394291877747\n",
      "EPOC 17 LOSS: 0.3639371395111084\n",
      "EPOC 18 LOSS: 0.35028141736984253\n",
      "EPOC 19 LOSS: 0.3799446225166321\n",
      "MF validation NDCG: 0.07909590592710077\n",
      "\n",
      "\n",
      "MODEL: MLP LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 0.617647647857666\n",
      "EPOC 1 LOSS: 2.3255200386047363\n",
      "EPOC 2 LOSS: 0.4814028739929199\n",
      "EPOC 3 LOSS: 0.5679967999458313\n",
      "EPOC 4 LOSS: 0.5589310526847839\n",
      "EPOC 5 LOSS: 0.5463991761207581\n",
      "EPOC 6 LOSS: 0.5342056155204773\n",
      "EPOC 7 LOSS: 0.5228231549263\n",
      "EPOC 8 LOSS: 0.5123000741004944\n",
      "EPOC 9 LOSS: 0.50257807970047\n",
      "EPOC 10 LOSS: 0.49361109733581543\n",
      "EPOC 11 LOSS: 0.48539939522743225\n",
      "EPOC 12 LOSS: 0.477983683347702\n",
      "EPOC 13 LOSS: 0.4714251458644867\n",
      "EPOC 14 LOSS: 0.46578142046928406\n",
      "EPOC 15 LOSS: 0.461087703704834\n",
      "EPOC 16 LOSS: 0.4573453962802887\n",
      "EPOC 17 LOSS: 0.45451822876930237\n",
      "EPOC 18 LOSS: 0.45253485441207886\n",
      "EPOC 19 LOSS: 0.45129624009132385\n",
      "MLP validation NDCG: 0.060016886355492974\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# initializing data frame that will store the scores for\n",
    "# the 3 different algorithms\n",
    "scores_df = pd.DataFrame()\n",
    "# index of the data frame\n",
    "i = 0\n",
    "for model_name in ['GMF', 'MF', 'MLP']:\n",
    "    # model to device\n",
    "    model_dict[model_name][0] = model_dict[model_name][0].to(device)\n",
    "    # criterion to device\n",
    "    model_dict[model_name][1] = model_dict[model_name][1].to(device)\n",
    "    # we train each model using 20 epocs, the data containing\n",
    "    # positive and negative samples and a learning rate of 0.1\n",
    "    train_epocs(model_name, negative_samples_df, 20, lr=0.1)\n",
    "    # calculating the validation score for each model, using\n",
    "    # ktop = 15 as required by the task\n",
    "    ndcg_score = evaluate(model_name, val_loader, use_cuda, 15)\n",
    "    scores_df.loc[i, 'Model'] = model_name\n",
    "    scores_df.loc[i, 'negative_samples_ratio'] = 5\n",
    "    scores_df.loc[i, 'NDGC'] = ndcg_score\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RF8oG76c86cD"
   },
   "source": [
    "### Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "McgkDxf07qKT",
    "outputId": "38982552-1491-4a6e-f5d2-32a97bccd769"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>negative_samples_ratio</th>\n",
       "      <th>NDGC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GMF</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.061684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MF</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.079096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.060017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  negative_samples_ratio      NDGC\n",
       "0   GMF                     5.0  0.061684\n",
       "1    MF                     5.0  0.079096\n",
       "2   MLP                     5.0  0.060017"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQJ0TzCr9IXh"
   },
   "source": [
    "We can see that the performance of the MF model is better than the other 2, for the basic configuration. We are going then to continue working only with this model in order to try to enhance its performance. The good thing about this model, apart from its superior performance in this case, is that it's easier to include to it the rest of the data that we are given: pre-trained user information, pre-trained items information and social links between users. Because, until now, we have not used any more than the training data to build our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddw5HM7YKYMf"
   },
   "source": [
    "### Matrix factorization with users and items pre-trained data\n",
    "\n",
    "Let's first try to build a MF model including the pre-trained data for users and items (we shall include the social links in a further model and compare them). But how to do this?\n",
    "We know that in MF, the prediction is finally achieved by $U*V$, like in our previous basic MF model, where U and V are the embeddings of user and item respectively. But we could easily include the pre-trained features to the model by doing something like the following\n",
    "\n",
    "$(U + pre\\_trained\\_user)*(V + pre\\_trained\\_item)$.\n",
    "\n",
    "As we already know,  the first term of the product is still refering to user and the second one to item, so it would be OK. If we further develop the expression we will end up with the following cross-products at the time of making our prediction:\n",
    "\n",
    "$(U*V + U*pre\\_trained\\_item + pre\\_trained\\_user*V + pre\\_trained\\_user*pre\\_trained\\_item)$\n",
    "\n",
    "So let's go ahead and add all this relationships and see how it goes! But before that, one remark: as pre_trained_user and pre_trained_item, as stated by their names, are supposed to be pre_trained, we are not going to update their weights during the optimization (this will be traduced in the code as requires.grad = False).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixTA-U8-O0yS"
   },
   "outputs": [],
   "source": [
    "# we first convert the pre_trained data into tensors, to be able to include\n",
    "# it in our model\n",
    "users_df = users_df.rename(columns = {'Unnamed: 0':'user_id'})\n",
    "items_df = items_df.rename(columns = {'Unnamed: 0':'item_id'})\n",
    "# tensor for user\n",
    "users_tensor = torch.tensor(users_df.drop('user_id', axis=1).values.astype(np.float32))\n",
    "# tensor for item data\n",
    "items_tensor = torch.tensor(items_df.drop('item_id', axis=1).values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AF_AWL9OUZ3"
   },
   "outputs": [],
   "source": [
    "class MF_UI(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=256):\n",
    "        super(MF_UI, self).__init__()\n",
    "        # first part: same as basic MF\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.user_emb.weight.data.uniform_(0, 0.05)\n",
    "        self.item_emb.weight.data.uniform_(0, 0.05)\n",
    "        # here we initialize 2 new embeddings that we are going\n",
    "        # to fill up with the user and item tensors created above\n",
    "        self.pre_trained_users = nn.Embedding(num_users, emb_size)\n",
    "        self.pre_trained_items = nn.Embedding(num_items, emb_size)\n",
    "        # for user pre-trained data\n",
    "        self.pre_trained_users.weight=nn.Parameter(users_tensor)\n",
    "        # to avoid updating weights\n",
    "        self.pre_trained_users.weight.requires_grad = False\n",
    "        # for items data\n",
    "        self.pre_trained_items.weight=nn.Parameter(items_tensor)\n",
    "        # to avoid updating their weights\n",
    "        self.pre_trained_items.weight.requires_grad = False\n",
    "        \n",
    "    def forward(self, users, items):\n",
    "        # u and v are defined as basic MF\n",
    "        u = self.user_emb(users)\n",
    "        v = self.item_emb(items)\n",
    "        # pre_trained data\n",
    "        trained_users = self.pre_trained_users(users)\n",
    "        trained_items = self.pre_trained_items(items)\n",
    "        # developing the expression with all the cross-products explained above\n",
    "        ui = torch.sum(u * v, dim=1)\n",
    "        u_pretrained_i = torch.sum(u*trained_items, dim =1)\n",
    "        v_pretrained_u = torch.sum(v*trained_users, dim = 1)\n",
    "        pretrained_i_u = torch.sum(trained_users*trained_items, dim = 1)\n",
    "        # the prediction finally is the sum of all\n",
    "        pred = ui + u_pretrained_i + v_pretrained_u + pretrained_i_u\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbA4X_dUQAfO"
   },
   "source": [
    "### Matrix factorization with users, items and social data\n",
    "\n",
    "And what if we want to take this one step further and also include the social link data? We could do it analogously and then compare both models. Social link data is strictly refered to the users, so our \"user side\" of the product would now have 3 components, as follows:\n",
    "\n",
    "$(U + pre\\_trained\\_user + social\\_links)*(V + pre\\_trained\\_item)$\n",
    "\n",
    "which would yield:\n",
    "\n",
    "$U*V + U*pre\\_trained\\_item + pre\\_trained\\_user*V + pre\\_trained\\_user*pre\\_trained\\_item + social\\_links*V + social\\_links*pre\\_trained\\_item$\n",
    "\n",
    "Our social links data is a user-user interaction matrix of num_users*num_users. To be able to make it interact with the rest, we could factorize it into 2 matrices W and H with 256 factors each (to match with all the rest). Our prediction would expand then to the following:\n",
    "\n",
    "$U*V + U*pre\\_trained\\_item + pre\\_trained\\_user*V + pre\\_trained\\_user*pre\\_trained\\_item + W*V + W*pre\\_trained\\_item + H*V + H*pre\\_trained\\_item$\n",
    "\n",
    "So, before building our model, let's factorize the social links matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCypmyLASjHO"
   },
   "outputs": [],
   "source": [
    "# we are going to use the Non-Negative Matrix Factorization module from sklearn\n",
    "from sklearn.decomposition import NMF\n",
    "# first creating the sparse matrix of num_users*num_users because we are only\n",
    "# given the positive instances of the social links\n",
    "links_matrix_sparse = np.zeros((len(train_data.user_id.unique()),len(train_data.user_id.unique())))\n",
    "# storing the user relationships in both ways to be able to fill the sparse\n",
    "# matrix with ones in the corresponding coordinates\n",
    "links_src_des = list(zip(links_df.src, links_df.des))\n",
    "links_des_src = list(zip(links_df.des, links_df.src))\n",
    "# filling sparse matrix with users that are \"friends\"\n",
    "for index in links_src_des:\n",
    "  links_matrix_sparse[index] = 1\n",
    "for index in links_des_src:\n",
    "  links_matrix_sparse[index] = 1\n",
    "# decomposing the matrix into W and H\n",
    "model = NMF(n_components=256, init='random', random_state=0)\n",
    "W = model.fit_transform(links_matrix_sparse)\n",
    "H_t = np.transpose(model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFRr2cVqU3as"
   },
   "source": [
    "Now we are ready to build the model which is analogous to the one containing only the user and item interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAR5diOtU6OV"
   },
   "outputs": [],
   "source": [
    "class MF_UI_LINKS(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=256):\n",
    "        super(MF_UI_LINKS, self).__init__()\n",
    "        # same thing as before but we have to initialize 2 extra matrices:\n",
    "        # W and H\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.user_emb.weight.data.uniform_(0, 0.05)\n",
    "        self.item_emb.weight.data.uniform_(0, 0.05)\n",
    "        self.pre_trained_users = nn.Embedding(num_users, emb_size)\n",
    "        self.pre_trained_items = nn.Embedding(num_items, emb_size)\n",
    "        self.pre_trained_users.weight=nn.Parameter(users_tensor)\n",
    "        self.pre_trained_users.weight.requires_grad = False\n",
    "        self.pre_trained_items.weight=nn.Parameter(items_tensor)\n",
    "        self.pre_trained_items.weight.requires_grad = False\n",
    "        self.links_W = nn.Embedding(num_users, emb_size)\n",
    "        self.links_H = nn.Embedding(num_users, emb_size)\n",
    "        self.links_W.weight=nn.Parameter(torch.tensor(W))\n",
    "        self.links_W.weight.requires_grad = False\n",
    "        self.links_H.weight=nn.Parameter(torch.tensor(H_t))\n",
    "        self.links_H.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        # we add the interactions specified above\n",
    "        u = self.user_emb(users)\n",
    "        v = self.item_emb(items)\n",
    "        trained_users = self.pre_trained_users(users)\n",
    "        trained_items = self.pre_trained_items(items)\n",
    "        links_W = self.links_W(users)\n",
    "        links_H = self.links_H(users)\n",
    "        ui = torch.sum(u * v, dim=1)\n",
    "        u_pretrained_i = torch.sum(u*trained_items, dim =1)\n",
    "        v_pretrained_u = torch.sum(v*trained_users, dim = 1)\n",
    "        pretrained_i_u = torch.sum(trained_users*trained_items, dim = 1)\n",
    "        links_W_items = torch.sum(v*links_W, dim = 1)\n",
    "        links_W_pretrained_i = torch.sum(links_W*trained_items, dim = 1)\n",
    "        links_H_items = torch.sum(v*links_H, dim = 1)\n",
    "        links_H_pretrained_i = torch.sum(links_H*trained_items, dim = 1)\n",
    "        pred = ui + u_pretrained_i + v_pretrained_u + pretrained_i_u + \\\n",
    "        links_W_items + links_W_pretrained_i + links_H_items + links_H_pretrained_i\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HeVdTTRYQ7e"
   },
   "source": [
    "### Training and evaluating the models\n",
    "\n",
    "Now that we have the 2 models let's train them and compare their performances with the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZCTmxEKmbrb"
   },
   "outputs": [],
   "source": [
    "# initialize the model with pretrained user and item\n",
    "model_MF_UI = MF_UI(n_users, n_items, 256)\n",
    "criterion_MF_UI = nn.BCEWithLogitsLoss()\n",
    "# initialize the model with pretrained user, item and social links\n",
    "model_MF_UI_LINK = MF_UI_LINKS(n_users, n_items, 256)\n",
    "criterion_MF_UI_LINK = nn.BCEWithLogitsLoss()\n",
    "# we store our models in the same dictionary we were working on with the\n",
    "# basic models\n",
    "model_dict['MF_UI'] = [model_MF_UI, criterion_MF_UI]\n",
    "model_dict['MF_UI_LINK'] = [model_MF_UI_LINK, criterion_MF_UI_LINK]\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJ7dGsQjmGYU"
   },
   "outputs": [],
   "source": [
    "# we define another function to train and evaluate the models from now on,\n",
    "# in order not to repeat code over and over\n",
    "def train_evaluate_models(models, negative_samples, index_df, wd):\n",
    "    # arguments:\n",
    "    # models: list of all the models we want to train and evaluate\n",
    "    # negative_samples: the negative_instances/positive_instances ratio\n",
    "    # index_df: last index in the scores_df data frame, to be able to append the\n",
    "    # performance to the ones for other models that we already calculated\n",
    "    # wd is weight of decay, L2 regularization parameter\n",
    "    # we run the negative samples function to obtain the input data with the\n",
    "    # specified ratio\n",
    "    negative_samples_df = negative_sampler(negative_samples)\n",
    "    for model_name in models:\n",
    "        if use_cuda:\n",
    "            model_dict[model_name] = [model_dict[model_name][0].cuda(), model_dict[model_name][1]]\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # model to device\n",
    "        model_dict[model_name][0] = model_dict[model_name][0].to(device)\n",
    "        model_dict[model_name][1] = model_dict[model_name][1].to(device)\n",
    "        # we train each model using the data containing positive and negative\n",
    "        # but let's gradually decrease the learning rates, for our model to be able\n",
    "        # to refine itself taking smaller steps towards the opposite direction of the\n",
    "        # gradient as the training increases\n",
    "        train_epocs(model_name, negative_samples_df, 20, lr=0.1, wd = wd)\n",
    "        train_epocs(model_name, negative_samples_df, 20, lr=0.05, wd = wd)\n",
    "        train_epocs(model_name, negative_samples_df, 25, lr=0.01, wd = wd)\n",
    "        train_epocs(model_name, negative_samples_df, 25, lr=0.001, wd = wd)\n",
    "        # calculating the validation score for each model, using\n",
    "        # ktop = 15 as required by the task\n",
    "        ndcg_score = evaluate(model_name, val_loader, use_cuda, 15)\n",
    "        scores_df.loc[index_df, 'Model'] = model_name\n",
    "        scores_df.loc[index_df, 'negative_samples_ratio'] = negative_samples\n",
    "        scores_df.loc[index_df, 'NDGC'] = ndcg_score\n",
    "        index_df+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4umhmsBSnJbf",
    "outputId": "bf91180d-1620-4741-e227-f10ae6ac928b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: MF_UI LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 433.7292175292969\n",
      "EPOC 1 LOSS: 374.7277526855469\n",
      "EPOC 2 LOSS: 320.4078063964844\n",
      "EPOC 3 LOSS: 270.92572021484375\n",
      "EPOC 4 LOSS: 226.3919219970703\n",
      "EPOC 5 LOSS: 186.9141845703125\n",
      "EPOC 6 LOSS: 152.4609375\n",
      "EPOC 7 LOSS: 122.86993408203125\n",
      "EPOC 8 LOSS: 97.80474090576172\n",
      "EPOC 9 LOSS: 76.81956481933594\n",
      "EPOC 10 LOSS: 59.47769546508789\n",
      "EPOC 11 LOSS: 45.40359115600586\n",
      "EPOC 12 LOSS: 34.32390594482422\n",
      "EPOC 13 LOSS: 26.091543197631836\n",
      "EPOC 14 LOSS: 20.641616821289062\n",
      "EPOC 15 LOSS: 17.64630889892578\n",
      "EPOC 16 LOSS: 16.65658950805664\n",
      "EPOC 17 LOSS: 16.994243621826172\n",
      "EPOC 18 LOSS: 18.024377822875977\n",
      "EPOC 19 LOSS: 19.233539581298828\n",
      "MODEL: MF_UI LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 20.28384017944336\n",
      "EPOC 1 LOSS: 14.913393020629883\n",
      "EPOC 2 LOSS: 10.447698593139648\n",
      "EPOC 3 LOSS: 6.953083515167236\n",
      "EPOC 4 LOSS: 4.5775146484375\n",
      "EPOC 5 LOSS: 3.410247325897217\n",
      "EPOC 6 LOSS: 3.124905824661255\n",
      "EPOC 7 LOSS: 3.1906793117523193\n",
      "EPOC 8 LOSS: 3.2039833068847656\n",
      "EPOC 9 LOSS: 3.0266036987304688\n",
      "EPOC 10 LOSS: 2.6792731285095215\n",
      "EPOC 11 LOSS: 2.2363176345825195\n",
      "EPOC 12 LOSS: 1.777288556098938\n",
      "EPOC 13 LOSS: 1.3692134618759155\n",
      "EPOC 14 LOSS: 1.05222749710083\n",
      "EPOC 15 LOSS: 0.8561905026435852\n",
      "EPOC 16 LOSS: 0.7609336376190186\n",
      "EPOC 17 LOSS: 0.7197921872138977\n",
      "EPOC 18 LOSS: 0.698685348033905\n",
      "EPOC 19 LOSS: 0.6748694777488708\n",
      "MODEL: MF_UI LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.6427944898605347\n",
      "EPOC 1 LOSS: 0.5253011584281921\n",
      "EPOC 2 LOSS: 0.4427434206008911\n",
      "EPOC 3 LOSS: 0.3853369951248169\n",
      "EPOC 4 LOSS: 0.3449702262878418\n",
      "EPOC 5 LOSS: 0.31584835052490234\n",
      "EPOC 6 LOSS: 0.294717401266098\n",
      "EPOC 7 LOSS: 0.27976852655410767\n",
      "EPOC 8 LOSS: 0.2708168923854828\n",
      "EPOC 9 LOSS: 0.26547446846961975\n",
      "EPOC 10 LOSS: 0.2590106427669525\n",
      "EPOC 11 LOSS: 0.24848519265651703\n",
      "EPOC 12 LOSS: 0.23466546833515167\n",
      "EPOC 13 LOSS: 0.2217957228422165\n",
      "EPOC 14 LOSS: 0.21230918169021606\n",
      "EPOC 15 LOSS: 0.20532357692718506\n",
      "EPOC 16 LOSS: 0.199571892619133\n",
      "EPOC 17 LOSS: 0.19444657862186432\n",
      "EPOC 18 LOSS: 0.18958468735218048\n",
      "EPOC 19 LOSS: 0.18468955159187317\n",
      "EPOC 20 LOSS: 0.1796359419822693\n",
      "EPOC 21 LOSS: 0.17434972524642944\n",
      "EPOC 22 LOSS: 0.16887794435024261\n",
      "EPOC 23 LOSS: 0.1633380800485611\n",
      "EPOC 24 LOSS: 0.15790030360221863\n",
      "MODEL: MF_UI LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.15268000960350037\n",
      "EPOC 1 LOSS: 0.15079933404922485\n",
      "EPOC 2 LOSS: 0.1491602212190628\n",
      "EPOC 3 LOSS: 0.1477167159318924\n",
      "EPOC 4 LOSS: 0.1464310586452484\n",
      "EPOC 5 LOSS: 0.1452692598104477\n",
      "EPOC 6 LOSS: 0.14420270919799805\n",
      "EPOC 7 LOSS: 0.14320722222328186\n",
      "EPOC 8 LOSS: 0.14226141571998596\n",
      "EPOC 9 LOSS: 0.14134804904460907\n",
      "EPOC 10 LOSS: 0.14045476913452148\n",
      "EPOC 11 LOSS: 0.1395733803510666\n",
      "EPOC 12 LOSS: 0.1386992186307907\n",
      "EPOC 13 LOSS: 0.1378302276134491\n",
      "EPOC 14 LOSS: 0.1369660347700119\n",
      "EPOC 15 LOSS: 0.1361071765422821\n",
      "EPOC 16 LOSS: 0.1352548748254776\n",
      "EPOC 17 LOSS: 0.13441041111946106\n",
      "EPOC 18 LOSS: 0.13357479870319366\n",
      "EPOC 19 LOSS: 0.132748544216156\n",
      "EPOC 20 LOSS: 0.13193149864673615\n",
      "EPOC 21 LOSS: 0.13112309575080872\n",
      "EPOC 22 LOSS: 0.13032236695289612\n",
      "EPOC 23 LOSS: 0.12952826917171478\n",
      "EPOC 24 LOSS: 0.1287398487329483\n",
      "MF_UI validation NDCG: 0.2146173548525239\n",
      "\n",
      "\n",
      "MODEL: MF_UI_LINK LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 425.9277038574219\n",
      "EPOC 1 LOSS: 367.5001525878906\n",
      "EPOC 2 LOSS: 313.7523498535156\n",
      "EPOC 3 LOSS: 264.8367614746094\n",
      "EPOC 4 LOSS: 220.85987854003906\n",
      "EPOC 5 LOSS: 181.9195098876953\n",
      "EPOC 6 LOSS: 147.97523498535156\n",
      "EPOC 7 LOSS: 118.85707092285156\n",
      "EPOC 8 LOSS: 94.22415924072266\n",
      "EPOC 9 LOSS: 73.63676452636719\n",
      "EPOC 10 LOSS: 56.671329498291016\n",
      "EPOC 11 LOSS: 42.98577117919922\n",
      "EPOC 12 LOSS: 32.35862350463867\n",
      "EPOC 13 LOSS: 24.680734634399414\n",
      "EPOC 14 LOSS: 19.8262996673584\n",
      "EPOC 15 LOSS: 17.372339248657227\n",
      "EPOC 16 LOSS: 16.779041290283203\n",
      "EPOC 17 LOSS: 17.3361873626709\n",
      "EPOC 18 LOSS: 18.434289932250977\n",
      "EPOC 19 LOSS: 19.617212295532227\n",
      "MODEL: MF_UI_LINK LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 20.58563995361328\n",
      "EPOC 1 LOSS: 15.202902793884277\n",
      "EPOC 2 LOSS: 10.720670700073242\n",
      "EPOC 3 LOSS: 7.183724403381348\n",
      "EPOC 4 LOSS: 4.70943021774292\n",
      "EPOC 5 LOSS: 3.393256425857544\n",
      "EPOC 6 LOSS: 3.016479015350342\n",
      "EPOC 7 LOSS: 3.0640711784362793\n",
      "EPOC 8 LOSS: 3.124431848526001\n",
      "EPOC 9 LOSS: 3.002549648284912\n",
      "EPOC 10 LOSS: 2.6939587593078613\n",
      "EPOC 11 LOSS: 2.2666616439819336\n",
      "EPOC 12 LOSS: 1.803575873374939\n",
      "EPOC 13 LOSS: 1.380362629890442\n",
      "EPOC 14 LOSS: 1.049032211303711\n",
      "EPOC 15 LOSS: 0.834118127822876\n",
      "EPOC 16 LOSS: 0.7288818955421448\n",
      "EPOC 17 LOSS: 0.6848491430282593\n",
      "EPOC 18 LOSS: 0.6674866080284119\n",
      "EPOC 19 LOSS: 0.6520128846168518\n",
      "MODEL: MF_UI_LINK LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.629181444644928\n",
      "EPOC 1 LOSS: 0.5178709030151367\n",
      "EPOC 2 LOSS: 0.4368928372859955\n",
      "EPOC 3 LOSS: 0.3777121305465698\n",
      "EPOC 4 LOSS: 0.3332771360874176\n",
      "EPOC 5 LOSS: 0.29937654733657837\n",
      "EPOC 6 LOSS: 0.27392175793647766\n",
      "EPOC 7 LOSS: 0.2563050389289856\n",
      "EPOC 8 LOSS: 0.2460305392742157\n",
      "EPOC 9 LOSS: 0.2401573657989502\n",
      "EPOC 10 LOSS: 0.23316851258277893\n",
      "EPOC 11 LOSS: 0.22126197814941406\n",
      "EPOC 12 LOSS: 0.20560789108276367\n",
      "EPOC 13 LOSS: 0.1906781941652298\n",
      "EPOC 14 LOSS: 0.17914173007011414\n",
      "EPOC 15 LOSS: 0.1706552654504776\n",
      "EPOC 16 LOSS: 0.1640191376209259\n",
      "EPOC 17 LOSS: 0.15828955173492432\n",
      "EPOC 18 LOSS: 0.1528821885585785\n",
      "EPOC 19 LOSS: 0.1474209576845169\n",
      "EPOC 20 LOSS: 0.1417088806629181\n",
      "EPOC 21 LOSS: 0.1356838196516037\n",
      "EPOC 22 LOSS: 0.1294334977865219\n",
      "EPOC 23 LOSS: 0.12313186377286911\n",
      "EPOC 24 LOSS: 0.11699039489030838\n",
      "MODEL: MF_UI_LINK LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.11121698468923569\n",
      "EPOC 1 LOSS: 0.10941338539123535\n",
      "EPOC 2 LOSS: 0.10782698541879654\n",
      "EPOC 3 LOSS: 0.10642197728157043\n",
      "EPOC 4 LOSS: 0.10516417026519775\n",
      "EPOC 5 LOSS: 0.10402361303567886\n",
      "EPOC 6 LOSS: 0.10297218710184097\n",
      "EPOC 7 LOSS: 0.1019841656088829\n",
      "EPOC 8 LOSS: 0.10103794187307358\n",
      "EPOC 9 LOSS: 0.10011717677116394\n",
      "EPOC 10 LOSS: 0.0992114469408989\n",
      "EPOC 11 LOSS: 0.0983152687549591\n",
      "EPOC 12 LOSS: 0.09742672741413116\n",
      "EPOC 13 LOSS: 0.09654553234577179\n",
      "EPOC 14 LOSS: 0.09567201882600784\n",
      "EPOC 15 LOSS: 0.09480684250593185\n",
      "EPOC 16 LOSS: 0.0939508005976677\n",
      "EPOC 17 LOSS: 0.09310468286275864\n",
      "EPOC 18 LOSS: 0.09226890653371811\n",
      "EPOC 19 LOSS: 0.09144360572099686\n",
      "EPOC 20 LOSS: 0.09062853455543518\n",
      "EPOC 21 LOSS: 0.0898231491446495\n",
      "EPOC 22 LOSS: 0.08902666717767715\n",
      "EPOC 23 LOSS: 0.08823833614587784\n",
      "EPOC 24 LOSS: 0.08745743334293365\n",
      "MF_UI_LINK validation NDCG: 0.21495577294195248\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we are going to compare the 2 models from above, using 7 as the ratio\n",
    "# negative_samples/positive_samples\n",
    "train_evaluate_models(['MF_UI', 'MF_UI_LINK'], 7, len(scores_df) , wd = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOyk0aif-skm"
   },
   "source": [
    "### Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "BIX0x9BG1QBN",
    "outputId": "c71dd74b-2704-4e8c-ebc9-e7b5e3a8af6f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>negative_samples_ratio</th>\n",
       "      <th>NDGC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GMF</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.061684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MF</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.079096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.060017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MF_UI</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.214617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MF_UI_LINK</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.214956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  negative_samples_ratio      NDGC\n",
       "0         GMF                     5.0  0.061684\n",
       "1          MF                     5.0  0.079096\n",
       "2         MLP                     5.0  0.060017\n",
       "3       MF_UI                     7.0  0.214617\n",
       "4  MF_UI_LINK                     7.0  0.214956"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGsBniWc0fyR"
   },
   "source": [
    "We can see that the performances of both new models improved considerably when compared to the old models, and are almost the same when compared to each other, but MF_UI is a simpler model, which is always better. That is why we are going to proceed with this model for further tuning.\n",
    "\n",
    "### MF with UI pretrained and biases\n",
    "\n",
    "We are going to add biases to the pre-selected model and see how it goes. The biases added will correspond to the users and the items. I believe it is good to represent this in our model because we definitely will have users that will have a tendence to like more images than others, as well as images that will be naturally more liked that others, independently of the interaction between the 2. \n",
    "The prediction calculation will be the same as in the MF_UI model but adding the 2 new terms for the biases. We present the model as follows, as a modification of the MF_UI class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HSQnqa-3ZW_"
   },
   "outputs": [],
   "source": [
    "class MF_UI_BIAS(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=256):\n",
    "        super(MF_UI_BIAS, self).__init__()\n",
    "        # Exactly the same as MF_UI but we add user bias and item bias\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        # user bias embedding\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        # item bias embedding\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        self.user_emb.weight.data.uniform_(0, 0.05)\n",
    "        self.item_emb.weight.data.uniform_(0, 0.05)\n",
    "        # we initialize the weights of the biases\n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.pre_trained_users = nn.Embedding(num_users, emb_size)\n",
    "        self.pre_trained_items = nn.Embedding(num_items, emb_size)\n",
    "        self.pre_trained_users.weight=nn.Parameter(users_tensor)\n",
    "        self.pre_trained_users.weight.requires_grad = False\n",
    "        self.pre_trained_items.weight=nn.Parameter(items_tensor)\n",
    "        self.pre_trained_items.weight.requires_grad = False\n",
    "        \n",
    "    def forward(self, users, items):\n",
    "        u = self.user_emb(users)\n",
    "        v = self.item_emb(items)\n",
    "        # biases\n",
    "        b_users = self.user_bias(users).squeeze()\n",
    "        b_items = self.item_bias(items).squeeze()\n",
    "        trained_users = self.pre_trained_users(users)\n",
    "        trained_items = self.pre_trained_items(items)\n",
    "        ui = torch.sum(u * v, dim=1)\n",
    "        u_pretrained_i = torch.sum(u*trained_items, dim =1)\n",
    "        v_pretrained_u = torch.sum(v*trained_users, dim = 1)\n",
    "        pretrained_i_u = torch.sum(trained_users*trained_items, dim = 1)\n",
    "        # we add them to the summation of all the other terms\n",
    "        pred = ui + u_pretrained_i + v_pretrained_u + pretrained_i_u + b_users + b_items \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRwBrv3O6iza"
   },
   "source": [
    "### Training and evaluating the models\n",
    "\n",
    "Now that we have defined the model with the biases, we could compare it with the one MF_UI without the biases, using different values for the negative_samples/positive_samples ratio and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkdGEJe466la",
    "outputId": "dab1e810-dcf6-499b-88a8-3fd133e8f8ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: MF_UI LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 434.3218688964844\n",
      "EPOC 1 LOSS: 375.2796936035156\n",
      "EPOC 2 LOSS: 320.920654296875\n",
      "EPOC 3 LOSS: 271.400390625\n",
      "EPOC 4 LOSS: 226.82066345214844\n",
      "EPOC 5 LOSS: 187.28656005859375\n",
      "EPOC 6 LOSS: 152.78294372558594\n",
      "EPOC 7 LOSS: 123.14726257324219\n",
      "EPOC 8 LOSS: 98.04100799560547\n",
      "EPOC 9 LOSS: 77.01679229736328\n",
      "EPOC 10 LOSS: 59.64283752441406\n",
      "EPOC 11 LOSS: 45.5445556640625\n",
      "EPOC 12 LOSS: 34.44348907470703\n",
      "EPOC 13 LOSS: 26.198272705078125\n",
      "EPOC 14 LOSS: 20.718915939331055\n",
      "EPOC 15 LOSS: 17.70241355895996\n",
      "EPOC 16 LOSS: 16.685121536254883\n",
      "EPOC 17 LOSS: 17.008817672729492\n",
      "EPOC 18 LOSS: 18.026330947875977\n",
      "EPOC 19 LOSS: 19.22974967956543\n",
      "MODEL: MF_UI LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 20.275278091430664\n",
      "EPOC 1 LOSS: 14.902205467224121\n",
      "EPOC 2 LOSS: 10.43221664428711\n",
      "EPOC 3 LOSS: 6.9371018409729\n",
      "EPOC 4 LOSS: 4.553884983062744\n",
      "EPOC 5 LOSS: 3.3823108673095703\n",
      "EPOC 6 LOSS: 3.1194376945495605\n",
      "EPOC 7 LOSS: 3.186985492706299\n",
      "EPOC 8 LOSS: 3.1980388164520264\n",
      "EPOC 9 LOSS: 3.0240631103515625\n",
      "EPOC 10 LOSS: 2.681816816329956\n",
      "EPOC 11 LOSS: 2.238624095916748\n",
      "EPOC 12 LOSS: 1.7784416675567627\n",
      "EPOC 13 LOSS: 1.3700746297836304\n",
      "EPOC 14 LOSS: 1.0559508800506592\n",
      "EPOC 15 LOSS: 0.8581554889678955\n",
      "EPOC 16 LOSS: 0.7578933835029602\n",
      "EPOC 17 LOSS: 0.7153518795967102\n",
      "EPOC 18 LOSS: 0.6949744820594788\n",
      "EPOC 19 LOSS: 0.6737262606620789\n",
      "MODEL: MF_UI LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.6433301568031311\n",
      "EPOC 1 LOSS: 0.5257229208946228\n",
      "EPOC 2 LOSS: 0.4433474838733673\n",
      "EPOC 3 LOSS: 0.38618597388267517\n",
      "EPOC 4 LOSS: 0.34582698345184326\n",
      "EPOC 5 LOSS: 0.31645268201828003\n",
      "EPOC 6 LOSS: 0.2948251962661743\n",
      "EPOC 7 LOSS: 0.27994996309280396\n",
      "EPOC 8 LOSS: 0.2712361514568329\n",
      "EPOC 9 LOSS: 0.2660459280014038\n",
      "EPOC 10 LOSS: 0.2595498859882355\n",
      "EPOC 11 LOSS: 0.2488372027873993\n",
      "EPOC 12 LOSS: 0.23510703444480896\n",
      "EPOC 13 LOSS: 0.22223728895187378\n",
      "EPOC 14 LOSS: 0.21257475018501282\n",
      "EPOC 15 LOSS: 0.20550702512264252\n",
      "EPOC 16 LOSS: 0.1997947096824646\n",
      "EPOC 17 LOSS: 0.1947343647480011\n",
      "EPOC 18 LOSS: 0.1898745596408844\n",
      "EPOC 19 LOSS: 0.18492411077022552\n",
      "EPOC 20 LOSS: 0.1797831952571869\n",
      "EPOC 21 LOSS: 0.17445319890975952\n",
      "EPOC 22 LOSS: 0.16898256540298462\n",
      "EPOC 23 LOSS: 0.16346730291843414\n",
      "EPOC 24 LOSS: 0.158055379986763\n",
      "MODEL: MF_UI LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.15282787382602692\n",
      "EPOC 1 LOSS: 0.1509363353252411\n",
      "EPOC 2 LOSS: 0.1493009477853775\n",
      "EPOC 3 LOSS: 0.14787180721759796\n",
      "EPOC 4 LOSS: 0.14659953117370605\n",
      "EPOC 5 LOSS: 0.14544442296028137\n",
      "EPOC 6 LOSS: 0.14437739551067352\n",
      "EPOC 7 LOSS: 0.14337657392024994\n",
      "EPOC 8 LOSS: 0.14242443442344666\n",
      "EPOC 9 LOSS: 0.14150644838809967\n",
      "EPOC 10 LOSS: 0.14061129093170166\n",
      "EPOC 11 LOSS: 0.13973112404346466\n",
      "EPOC 12 LOSS: 0.1388607919216156\n",
      "EPOC 13 LOSS: 0.13799697160720825\n",
      "EPOC 14 LOSS: 0.13713780045509338\n",
      "EPOC 15 LOSS: 0.13628290593624115\n",
      "EPOC 16 LOSS: 0.13543301820755005\n",
      "EPOC 17 LOSS: 0.13458964228630066\n",
      "EPOC 18 LOSS: 0.13375400006771088\n",
      "EPOC 19 LOSS: 0.13292695581912994\n",
      "EPOC 20 LOSS: 0.132108673453331\n",
      "EPOC 21 LOSS: 0.13129886984825134\n",
      "EPOC 22 LOSS: 0.13049671053886414\n",
      "EPOC 23 LOSS: 0.1297014057636261\n",
      "EPOC 24 LOSS: 0.12891216576099396\n",
      "MF_UI validation NDCG: 0.20538404276533845\n",
      "\n",
      "\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 434.3277587890625\n",
      "EPOC 1 LOSS: 375.1101989746094\n",
      "EPOC 2 LOSS: 320.5774841308594\n",
      "EPOC 3 LOSS: 270.8886413574219\n",
      "EPOC 4 LOSS: 226.14804077148438\n",
      "EPOC 5 LOSS: 186.47030639648438\n",
      "EPOC 6 LOSS: 151.84458923339844\n",
      "EPOC 7 LOSS: 122.11067199707031\n",
      "EPOC 8 LOSS: 96.9290542602539\n",
      "EPOC 9 LOSS: 75.8397445678711\n",
      "EPOC 10 LOSS: 58.40793991088867\n",
      "EPOC 11 LOSS: 44.24432373046875\n",
      "EPOC 12 LOSS: 33.060707092285156\n",
      "EPOC 13 LOSS: 24.726865768432617\n",
      "EPOC 14 LOSS: 19.16838264465332\n",
      "EPOC 15 LOSS: 16.093904495239258\n",
      "EPOC 16 LOSS: 15.033236503601074\n",
      "EPOC 17 LOSS: 15.304558753967285\n",
      "EPOC 18 LOSS: 16.25265121459961\n",
      "EPOC 19 LOSS: 17.359621047973633\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 18.29707145690918\n",
      "EPOC 1 LOSS: 12.90612506866455\n",
      "EPOC 2 LOSS: 8.464848518371582\n",
      "EPOC 3 LOSS: 5.092123031616211\n",
      "EPOC 4 LOSS: 2.9781222343444824\n",
      "EPOC 5 LOSS: 2.1721603870391846\n",
      "EPOC 6 LOSS: 2.170416831970215\n",
      "EPOC 7 LOSS: 2.3588969707489014\n",
      "EPOC 8 LOSS: 2.4422378540039062\n",
      "EPOC 9 LOSS: 2.346912384033203\n",
      "EPOC 10 LOSS: 2.1058127880096436\n",
      "EPOC 11 LOSS: 1.7804819345474243\n",
      "EPOC 12 LOSS: 1.4417728185653687\n",
      "EPOC 13 LOSS: 1.1422698497772217\n",
      "EPOC 14 LOSS: 0.9194769859313965\n",
      "EPOC 15 LOSS: 0.7804949283599854\n",
      "EPOC 16 LOSS: 0.7101728320121765\n",
      "EPOC 17 LOSS: 0.680342972278595\n",
      "EPOC 18 LOSS: 0.670210599899292\n",
      "EPOC 19 LOSS: 0.6663802862167358\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.6619685888290405\n",
      "EPOC 1 LOSS: 0.5820658802986145\n",
      "EPOC 2 LOSS: 0.5204505324363708\n",
      "EPOC 3 LOSS: 0.47002968192100525\n",
      "EPOC 4 LOSS: 0.4272482097148895\n",
      "EPOC 5 LOSS: 0.39005473256111145\n",
      "EPOC 6 LOSS: 0.35805922746658325\n",
      "EPOC 7 LOSS: 0.3312971293926239\n",
      "EPOC 8 LOSS: 0.3102211356163025\n",
      "EPOC 9 LOSS: 0.2946065068244934\n",
      "EPOC 10 LOSS: 0.28390103578567505\n",
      "EPOC 11 LOSS: 0.2747657001018524\n",
      "EPOC 12 LOSS: 0.26242390275001526\n",
      "EPOC 13 LOSS: 0.24653896689414978\n",
      "EPOC 14 LOSS: 0.23068535327911377\n",
      "EPOC 15 LOSS: 0.2182513326406479\n",
      "EPOC 16 LOSS: 0.20952703058719635\n",
      "EPOC 17 LOSS: 0.2028130441904068\n",
      "EPOC 18 LOSS: 0.196603462100029\n",
      "EPOC 19 LOSS: 0.1901288777589798\n",
      "EPOC 20 LOSS: 0.1831609159708023\n",
      "EPOC 21 LOSS: 0.17576159536838531\n",
      "EPOC 22 LOSS: 0.1681460291147232\n",
      "EPOC 23 LOSS: 0.16059857606887817\n",
      "EPOC 24 LOSS: 0.153434619307518\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.1469959318637848\n",
      "EPOC 1 LOSS: 0.1451793909072876\n",
      "EPOC 2 LOSS: 0.14363695681095123\n",
      "EPOC 3 LOSS: 0.1422983705997467\n",
      "EPOC 4 LOSS: 0.14109477400779724\n",
      "EPOC 5 LOSS: 0.13997584581375122\n",
      "EPOC 6 LOSS: 0.13890965282917023\n",
      "EPOC 7 LOSS: 0.13787508010864258\n",
      "EPOC 8 LOSS: 0.13685834407806396\n",
      "EPOC 9 LOSS: 0.1358523815870285\n",
      "EPOC 10 LOSS: 0.13485509157180786\n",
      "EPOC 11 LOSS: 0.1338668167591095\n",
      "EPOC 12 LOSS: 0.1328887790441513\n",
      "EPOC 13 LOSS: 0.13192225992679596\n",
      "EPOC 14 LOSS: 0.1309681385755539\n",
      "EPOC 15 LOSS: 0.13002654910087585\n",
      "EPOC 16 LOSS: 0.12909677624702454\n",
      "EPOC 17 LOSS: 0.1281777322292328\n",
      "EPOC 18 LOSS: 0.12726803123950958\n",
      "EPOC 19 LOSS: 0.1263665407896042\n",
      "EPOC 20 LOSS: 0.12547247111797333\n",
      "EPOC 21 LOSS: 0.12458544969558716\n",
      "EPOC 22 LOSS: 0.12370532751083374\n",
      "EPOC 23 LOSS: 0.12283207476139069\n",
      "EPOC 24 LOSS: 0.12196569889783859\n",
      "MF_UI_BIAS validation NDCG: 0.21686107808175256\n",
      "\n",
      "\n",
      "MODEL: MF_UI LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 451.3623352050781\n",
      "EPOC 1 LOSS: 390.0104064941406\n",
      "EPOC 2 LOSS: 333.5194091796875\n",
      "EPOC 3 LOSS: 282.0508117675781\n",
      "EPOC 4 LOSS: 235.71484375\n",
      "EPOC 5 LOSS: 194.619384765625\n",
      "EPOC 6 LOSS: 158.73934936523438\n",
      "EPOC 7 LOSS: 127.9112777709961\n",
      "EPOC 8 LOSS: 101.79248809814453\n",
      "EPOC 9 LOSS: 79.92533111572266\n",
      "EPOC 10 LOSS: 61.86128234863281\n",
      "EPOC 11 LOSS: 47.211395263671875\n",
      "EPOC 12 LOSS: 35.68707275390625\n",
      "EPOC 13 LOSS: 27.132230758666992\n",
      "EPOC 14 LOSS: 21.450090408325195\n",
      "EPOC 15 LOSS: 18.320838928222656\n",
      "EPOC 16 LOSS: 17.258275985717773\n",
      "EPOC 17 LOSS: 17.582122802734375\n",
      "EPOC 18 LOSS: 18.64078712463379\n",
      "EPOC 19 LOSS: 19.910390853881836\n",
      "MODEL: MF_UI LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 21.033409118652344\n",
      "EPOC 1 LOSS: 15.499531745910645\n",
      "EPOC 2 LOSS: 10.877753257751465\n",
      "EPOC 3 LOSS: 7.247639179229736\n",
      "EPOC 4 LOSS: 4.759122371673584\n",
      "EPOC 5 LOSS: 3.4862115383148193\n",
      "EPOC 6 LOSS: 3.155038356781006\n",
      "EPOC 7 LOSS: 3.2242543697357178\n",
      "EPOC 8 LOSS: 3.256524085998535\n",
      "EPOC 9 LOSS: 3.0922482013702393\n",
      "EPOC 10 LOSS: 2.7530505657196045\n",
      "EPOC 11 LOSS: 2.3077406883239746\n",
      "EPOC 12 LOSS: 1.8380212783813477\n",
      "EPOC 13 LOSS: 1.4103821516036987\n",
      "EPOC 14 LOSS: 1.0715922117233276\n",
      "EPOC 15 LOSS: 0.8447416424751282\n",
      "EPOC 16 LOSS: 0.7139580845832825\n",
      "EPOC 17 LOSS: 0.651620090007782\n",
      "EPOC 18 LOSS: 0.621748685836792\n",
      "EPOC 19 LOSS: 0.5985287427902222\n",
      "MODEL: MF_UI LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.57193523645401\n",
      "EPOC 1 LOSS: 0.46395283937454224\n",
      "EPOC 2 LOSS: 0.39205029606819153\n",
      "EPOC 3 LOSS: 0.34423625469207764\n",
      "EPOC 4 LOSS: 0.3111725151538849\n",
      "EPOC 5 LOSS: 0.2868436276912689\n",
      "EPOC 6 LOSS: 0.268098384141922\n",
      "EPOC 7 LOSS: 0.2540496289730072\n",
      "EPOC 8 LOSS: 0.2449617087841034\n",
      "EPOC 9 LOSS: 0.23919963836669922\n",
      "EPOC 10 LOSS: 0.23417402803897858\n",
      "EPOC 11 LOSS: 0.22685745358467102\n",
      "EPOC 12 LOSS: 0.21671909093856812\n",
      "EPOC 13 LOSS: 0.20581220090389252\n",
      "EPOC 14 LOSS: 0.1967499554157257\n",
      "EPOC 15 LOSS: 0.1899368315935135\n",
      "EPOC 16 LOSS: 0.1844811886548996\n",
      "EPOC 17 LOSS: 0.17974382638931274\n",
      "EPOC 18 LOSS: 0.17540815472602844\n",
      "EPOC 19 LOSS: 0.1712585836648941\n",
      "EPOC 20 LOSS: 0.16713844239711761\n",
      "EPOC 21 LOSS: 0.16290390491485596\n",
      "EPOC 22 LOSS: 0.1585114747285843\n",
      "EPOC 23 LOSS: 0.1540524661540985\n",
      "EPOC 24 LOSS: 0.14963077008724213\n",
      "MODEL: MF_UI LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.14528758823871613\n",
      "EPOC 1 LOSS: 0.14360590279102325\n",
      "EPOC 2 LOSS: 0.14213736355304718\n",
      "EPOC 3 LOSS: 0.14083579182624817\n",
      "EPOC 4 LOSS: 0.13966473937034607\n",
      "EPOC 5 LOSS: 0.1385963261127472\n",
      "EPOC 6 LOSS: 0.13761164247989655\n",
      "EPOC 7 LOSS: 0.13669699430465698\n",
      "EPOC 8 LOSS: 0.1358400285243988\n",
      "EPOC 9 LOSS: 0.13502733409404755\n",
      "EPOC 10 LOSS: 0.1342455893754959\n",
      "EPOC 11 LOSS: 0.13348354399204254\n",
      "EPOC 12 LOSS: 0.13273261487483978\n",
      "EPOC 13 LOSS: 0.13198715448379517\n",
      "EPOC 14 LOSS: 0.13124459981918335\n",
      "EPOC 15 LOSS: 0.13050498068332672\n",
      "EPOC 16 LOSS: 0.1297699213027954\n",
      "EPOC 17 LOSS: 0.12904158234596252\n",
      "EPOC 18 LOSS: 0.12832164764404297\n",
      "EPOC 19 LOSS: 0.127610981464386\n",
      "EPOC 20 LOSS: 0.12690971791744232\n",
      "EPOC 21 LOSS: 0.126217320561409\n",
      "EPOC 22 LOSS: 0.12553276121616364\n",
      "EPOC 23 LOSS: 0.1248549148440361\n",
      "EPOC 24 LOSS: 0.12418246269226074\n",
      "MF_UI validation NDCG: 0.22976083352903717\n",
      "\n",
      "\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 451.3599548339844\n",
      "EPOC 1 LOSS: 389.8266906738281\n",
      "EPOC 2 LOSS: 333.15545654296875\n",
      "EPOC 3 LOSS: 281.5116882324219\n",
      "EPOC 4 LOSS: 235.00955200195312\n",
      "EPOC 5 LOSS: 193.7647247314453\n",
      "EPOC 6 LOSS: 157.7571563720703\n",
      "EPOC 7 LOSS: 126.8254165649414\n",
      "EPOC 8 LOSS: 100.62498474121094\n",
      "EPOC 9 LOSS: 78.68659210205078\n",
      "EPOC 10 LOSS: 60.559696197509766\n",
      "EPOC 11 LOSS: 45.841678619384766\n",
      "EPOC 12 LOSS: 34.232330322265625\n",
      "EPOC 13 LOSS: 25.584716796875\n",
      "EPOC 14 LOSS: 19.819114685058594\n",
      "EPOC 15 LOSS: 16.626371383666992\n",
      "EPOC 16 LOSS: 15.513099670410156\n",
      "EPOC 17 LOSS: 15.773646354675293\n",
      "EPOC 18 LOSS: 16.75578498840332\n",
      "EPOC 19 LOSS: 17.922367095947266\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 18.932363510131836\n",
      "EPOC 1 LOSS: 13.380082130432129\n",
      "EPOC 2 LOSS: 8.790345191955566\n",
      "EPOC 3 LOSS: 5.284726142883301\n",
      "EPOC 4 LOSS: 3.0685486793518066\n",
      "EPOC 5 LOSS: 2.1536684036254883\n",
      "EPOC 6 LOSS: 2.090998649597168\n",
      "EPOC 7 LOSS: 2.286893606185913\n",
      "EPOC 8 LOSS: 2.3970117568969727\n",
      "EPOC 9 LOSS: 2.325685739517212\n",
      "EPOC 10 LOSS: 2.1012139320373535\n",
      "EPOC 11 LOSS: 1.7843058109283447\n",
      "EPOC 12 LOSS: 1.4433960914611816\n",
      "EPOC 13 LOSS: 1.1309317350387573\n",
      "EPOC 14 LOSS: 0.8877514600753784\n",
      "EPOC 15 LOSS: 0.7261556386947632\n",
      "EPOC 16 LOSS: 0.6357450485229492\n",
      "EPOC 17 LOSS: 0.5955134034156799\n",
      "EPOC 18 LOSS: 0.5818174481391907\n",
      "EPOC 19 LOSS: 0.5790212154388428\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.5781587362289429\n",
      "EPOC 1 LOSS: 0.508987545967102\n",
      "EPOC 2 LOSS: 0.4581298232078552\n",
      "EPOC 3 LOSS: 0.41760069131851196\n",
      "EPOC 4 LOSS: 0.38304680585861206\n",
      "EPOC 5 LOSS: 0.3528306782245636\n",
      "EPOC 6 LOSS: 0.3261273205280304\n",
      "EPOC 7 LOSS: 0.3026391267776489\n",
      "EPOC 8 LOSS: 0.28269293904304504\n",
      "EPOC 9 LOSS: 0.2671995162963867\n",
      "EPOC 10 LOSS: 0.25602319836616516\n",
      "EPOC 11 LOSS: 0.24776916205883026\n",
      "EPOC 12 LOSS: 0.23857246339321136\n",
      "EPOC 13 LOSS: 0.22643806040287018\n",
      "EPOC 14 LOSS: 0.2133292555809021\n",
      "EPOC 15 LOSS: 0.2022193819284439\n",
      "EPOC 16 LOSS: 0.1939927488565445\n",
      "EPOC 17 LOSS: 0.1875889003276825\n",
      "EPOC 18 LOSS: 0.18186667561531067\n",
      "EPOC 19 LOSS: 0.17623305320739746\n",
      "EPOC 20 LOSS: 0.17041897773742676\n",
      "EPOC 21 LOSS: 0.1643689125776291\n",
      "EPOC 22 LOSS: 0.15816724300384521\n",
      "EPOC 23 LOSS: 0.1519407033920288\n",
      "EPOC 24 LOSS: 0.1458413302898407\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.14015531539916992\n",
      "EPOC 1 LOSS: 0.1385083645582199\n",
      "EPOC 2 LOSS: 0.13710878789424896\n",
      "EPOC 3 LOSS: 0.13589154183864594\n",
      "EPOC 4 LOSS: 0.13480383157730103\n",
      "EPOC 5 LOSS: 0.13380500674247742\n",
      "EPOC 6 LOSS: 0.13286463916301727\n",
      "EPOC 7 LOSS: 0.13196049630641937\n",
      "EPOC 8 LOSS: 0.131077840924263\n",
      "EPOC 9 LOSS: 0.13020755350589752\n",
      "EPOC 10 LOSS: 0.1293448507785797\n",
      "EPOC 11 LOSS: 0.12848812341690063\n",
      "EPOC 12 LOSS: 0.12763869762420654\n",
      "EPOC 13 LOSS: 0.126798614859581\n",
      "EPOC 14 LOSS: 0.12596920132637024\n",
      "EPOC 15 LOSS: 0.1251508593559265\n",
      "EPOC 16 LOSS: 0.12434322386980057\n",
      "EPOC 17 LOSS: 0.12354534864425659\n",
      "EPOC 18 LOSS: 0.12275620549917221\n",
      "EPOC 19 LOSS: 0.12197495251893997\n",
      "EPOC 20 LOSS: 0.12120100110769272\n",
      "EPOC 21 LOSS: 0.12043390423059464\n",
      "EPOC 22 LOSS: 0.11967325955629349\n",
      "EPOC 23 LOSS: 0.11891888082027435\n",
      "EPOC 24 LOSS: 0.11817049235105515\n",
      "MF_UI_BIAS validation NDCG: 0.23150778356354548\n",
      "\n",
      "\n",
      "MODEL: MF_UI LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 465.060546875\n",
      "EPOC 1 LOSS: 401.8247375488281\n",
      "EPOC 2 LOSS: 343.5999755859375\n",
      "EPOC 3 LOSS: 290.5514221191406\n",
      "EPOC 4 LOSS: 242.78662109375\n",
      "EPOC 5 LOSS: 200.41847229003906\n",
      "EPOC 6 LOSS: 163.4217071533203\n",
      "EPOC 7 LOSS: 131.62887573242188\n",
      "EPOC 8 LOSS: 104.69883728027344\n",
      "EPOC 9 LOSS: 82.16173553466797\n",
      "EPOC 10 LOSS: 63.555747985839844\n",
      "EPOC 11 LOSS: 48.48321533203125\n",
      "EPOC 12 LOSS: 36.63755798339844\n",
      "EPOC 13 LOSS: 27.84620475769043\n",
      "EPOC 14 LOSS: 22.0134334564209\n",
      "EPOC 15 LOSS: 18.800060272216797\n",
      "EPOC 16 LOSS: 17.706974029541016\n",
      "EPOC 17 LOSS: 18.03970718383789\n",
      "EPOC 18 LOSS: 19.136383056640625\n",
      "EPOC 19 LOSS: 20.45079803466797\n",
      "MODEL: MF_UI LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 21.61429214477539\n",
      "EPOC 1 LOSS: 15.953590393066406\n",
      "EPOC 2 LOSS: 11.20927906036377\n",
      "EPOC 3 LOSS: 7.462778568267822\n",
      "EPOC 4 LOSS: 4.8739213943481445\n",
      "EPOC 5 LOSS: 3.5341129302978516\n",
      "EPOC 6 LOSS: 3.1810293197631836\n",
      "EPOC 7 LOSS: 3.2667386531829834\n",
      "EPOC 8 LOSS: 3.302919626235962\n",
      "EPOC 9 LOSS: 3.139324188232422\n",
      "EPOC 10 LOSS: 2.7960283756256104\n",
      "EPOC 11 LOSS: 2.3433358669281006\n",
      "EPOC 12 LOSS: 1.8648505210876465\n",
      "EPOC 13 LOSS: 1.4243676662445068\n",
      "EPOC 14 LOSS: 1.0675568580627441\n",
      "EPOC 15 LOSS: 0.8198741674423218\n",
      "EPOC 16 LOSS: 0.6648985743522644\n",
      "EPOC 17 LOSS: 0.5816743969917297\n",
      "EPOC 18 LOSS: 0.5367859601974487\n",
      "EPOC 19 LOSS: 0.5069214105606079\n",
      "MODEL: MF_UI LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.4804830849170685\n",
      "EPOC 1 LOSS: 0.3851034939289093\n",
      "EPOC 2 LOSS: 0.3251664340496063\n",
      "EPOC 3 LOSS: 0.2872626483440399\n",
      "EPOC 4 LOSS: 0.26159462332725525\n",
      "EPOC 5 LOSS: 0.24254167079925537\n",
      "EPOC 6 LOSS: 0.227959543466568\n",
      "EPOC 7 LOSS: 0.2166745513677597\n",
      "EPOC 8 LOSS: 0.2082163542509079\n",
      "EPOC 9 LOSS: 0.20241519808769226\n",
      "EPOC 10 LOSS: 0.19784095883369446\n",
      "EPOC 11 LOSS: 0.1931285709142685\n",
      "EPOC 12 LOSS: 0.1867617964744568\n",
      "EPOC 13 LOSS: 0.17856669425964355\n",
      "EPOC 14 LOSS: 0.17060765624046326\n",
      "EPOC 15 LOSS: 0.1644337773323059\n",
      "EPOC 16 LOSS: 0.15965412557125092\n",
      "EPOC 17 LOSS: 0.15559162199497223\n",
      "EPOC 18 LOSS: 0.1519077569246292\n",
      "EPOC 19 LOSS: 0.14844723045825958\n",
      "EPOC 20 LOSS: 0.14514605700969696\n",
      "EPOC 21 LOSS: 0.141901895403862\n",
      "EPOC 22 LOSS: 0.1386088877916336\n",
      "EPOC 23 LOSS: 0.13525927066802979\n",
      "EPOC 24 LOSS: 0.1318945288658142\n",
      "MODEL: MF_UI LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.12860387563705444\n",
      "EPOC 1 LOSS: 0.12725979089736938\n",
      "EPOC 2 LOSS: 0.12607580423355103\n",
      "EPOC 3 LOSS: 0.12501148879528046\n",
      "EPOC 4 LOSS: 0.12403891235589981\n",
      "EPOC 5 LOSS: 0.12314005941152573\n",
      "EPOC 6 LOSS: 0.12230388820171356\n",
      "EPOC 7 LOSS: 0.12152254581451416\n",
      "EPOC 8 LOSS: 0.12078887224197388\n",
      "EPOC 9 LOSS: 0.12009543180465698\n",
      "EPOC 10 LOSS: 0.11943387240171432\n",
      "EPOC 11 LOSS: 0.11879543960094452\n",
      "EPOC 12 LOSS: 0.11817186325788498\n",
      "EPOC 13 LOSS: 0.11755675822496414\n",
      "EPOC 14 LOSS: 0.11694646626710892\n",
      "EPOC 15 LOSS: 0.1163397878408432\n",
      "EPOC 16 LOSS: 0.11573740094900131\n",
      "EPOC 17 LOSS: 0.11514067649841309\n",
      "EPOC 18 LOSS: 0.11455117911100388\n",
      "EPOC 19 LOSS: 0.11397010833024979\n",
      "EPOC 20 LOSS: 0.1133980080485344\n",
      "EPOC 21 LOSS: 0.11283465474843979\n",
      "EPOC 22 LOSS: 0.1122792661190033\n",
      "EPOC 23 LOSS: 0.11173084378242493\n",
      "EPOC 24 LOSS: 0.11118831485509872\n",
      "MF_UI validation NDCG: 0.23547246789485515\n",
      "\n",
      "\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 465.0411071777344\n",
      "EPOC 1 LOSS: 401.61920166015625\n",
      "EPOC 2 LOSS: 343.2099304199219\n",
      "EPOC 3 LOSS: 289.9811096191406\n",
      "EPOC 4 LOSS: 242.04566955566406\n",
      "EPOC 5 LOSS: 199.5235137939453\n",
      "EPOC 6 LOSS: 162.39447021484375\n",
      "EPOC 7 LOSS: 130.49412536621094\n",
      "EPOC 8 LOSS: 103.48003387451172\n",
      "EPOC 9 LOSS: 80.86898803710938\n",
      "EPOC 10 LOSS: 62.19765090942383\n",
      "EPOC 11 LOSS: 47.05309295654297\n",
      "EPOC 12 LOSS: 35.11827087402344\n",
      "EPOC 13 LOSS: 26.231252670288086\n",
      "EPOC 14 LOSS: 20.313966751098633\n",
      "EPOC 15 LOSS: 17.031496047973633\n",
      "EPOC 16 LOSS: 15.880566596984863\n",
      "EPOC 17 LOSS: 16.144460678100586\n",
      "EPOC 18 LOSS: 17.15976333618164\n",
      "EPOC 19 LOSS: 18.36704444885254\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 19.407724380493164\n",
      "EPOC 1 LOSS: 13.732772827148438\n",
      "EPOC 2 LOSS: 9.020744323730469\n",
      "EPOC 3 LOSS: 5.403404235839844\n",
      "EPOC 4 LOSS: 3.087728500366211\n",
      "EPOC 5 LOSS: 2.1065900325775146\n",
      "EPOC 6 LOSS: 2.0273048877716064\n",
      "EPOC 7 LOSS: 2.2357113361358643\n",
      "EPOC 8 LOSS: 2.3539962768554688\n",
      "EPOC 9 LOSS: 2.2890255451202393\n",
      "EPOC 10 LOSS: 2.0698680877685547\n",
      "EPOC 11 LOSS: 1.7552226781845093\n",
      "EPOC 12 LOSS: 1.412949800491333\n",
      "EPOC 13 LOSS: 1.0941262245178223\n",
      "EPOC 14 LOSS: 0.8382066488265991\n",
      "EPOC 15 LOSS: 0.6601807475090027\n",
      "EPOC 16 LOSS: 0.5526893138885498\n",
      "EPOC 17 LOSS: 0.5001243948936462\n",
      "EPOC 18 LOSS: 0.4789828956127167\n",
      "EPOC 19 LOSS: 0.47290271520614624\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.4723513722419739\n",
      "EPOC 1 LOSS: 0.4135209619998932\n",
      "EPOC 2 LOSS: 0.37311503291130066\n",
      "EPOC 3 LOSS: 0.342044472694397\n",
      "EPOC 4 LOSS: 0.3160760700702667\n",
      "EPOC 5 LOSS: 0.2932392954826355\n",
      "EPOC 6 LOSS: 0.2729637324810028\n",
      "EPOC 7 LOSS: 0.2551238238811493\n",
      "EPOC 8 LOSS: 0.23961278796195984\n",
      "EPOC 9 LOSS: 0.22658023238182068\n",
      "EPOC 10 LOSS: 0.21633155643939972\n",
      "EPOC 11 LOSS: 0.20825237035751343\n",
      "EPOC 12 LOSS: 0.20146766304969788\n",
      "EPOC 13 LOSS: 0.1934528797864914\n",
      "EPOC 14 LOSS: 0.18366608023643494\n",
      "EPOC 15 LOSS: 0.17433121800422668\n",
      "EPOC 16 LOSS: 0.167057603597641\n",
      "EPOC 17 LOSS: 0.16150791943073273\n",
      "EPOC 18 LOSS: 0.15666761994361877\n",
      "EPOC 19 LOSS: 0.15199941396713257\n",
      "EPOC 20 LOSS: 0.14731734991073608\n",
      "EPOC 21 LOSS: 0.14260105788707733\n",
      "EPOC 22 LOSS: 0.13791078329086304\n",
      "EPOC 23 LOSS: 0.13322971761226654\n",
      "EPOC 24 LOSS: 0.1286013126373291\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.12416491657495499\n",
      "EPOC 1 LOSS: 0.12277185171842575\n",
      "EPOC 2 LOSS: 0.12158086150884628\n",
      "EPOC 3 LOSS: 0.12054309993982315\n",
      "EPOC 4 LOSS: 0.1196146011352539\n",
      "EPOC 5 LOSS: 0.11876160651445389\n",
      "EPOC 6 LOSS: 0.11796233057975769\n",
      "EPOC 7 LOSS: 0.1172042265534401\n",
      "EPOC 8 LOSS: 0.11647805571556091\n",
      "EPOC 9 LOSS: 0.115775927901268\n",
      "EPOC 10 LOSS: 0.11509066820144653\n",
      "EPOC 11 LOSS: 0.11441590636968613\n",
      "EPOC 12 LOSS: 0.11374688893556595\n",
      "EPOC 13 LOSS: 0.11308115720748901\n",
      "EPOC 14 LOSS: 0.11241841316223145\n",
      "EPOC 15 LOSS: 0.11175958812236786\n",
      "EPOC 16 LOSS: 0.11110620945692062\n",
      "EPOC 17 LOSS: 0.11045973747968674\n",
      "EPOC 18 LOSS: 0.10982097685337067\n",
      "EPOC 19 LOSS: 0.10919010639190674\n",
      "EPOC 20 LOSS: 0.10856693238019943\n",
      "EPOC 21 LOSS: 0.10795088857412338\n",
      "EPOC 22 LOSS: 0.10734134167432785\n",
      "EPOC 23 LOSS: 0.1067376583814621\n",
      "EPOC 24 LOSS: 0.1061391532421112\n",
      "MF_UI_BIAS validation NDCG: 0.23749062527013845\n",
      "\n",
      "\n",
      "MODEL: MF_UI LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 468.32257080078125\n",
      "EPOC 1 LOSS: 404.63116455078125\n",
      "EPOC 2 LOSS: 345.9883728027344\n",
      "EPOC 3 LOSS: 292.55712890625\n",
      "EPOC 4 LOSS: 244.44686889648438\n",
      "EPOC 5 LOSS: 201.77383422851562\n",
      "EPOC 6 LOSS: 164.5120849609375\n",
      "EPOC 7 LOSS: 132.4912872314453\n",
      "EPOC 8 LOSS: 105.367431640625\n",
      "EPOC 9 LOSS: 82.6690673828125\n",
      "EPOC 10 LOSS: 63.93583297729492\n",
      "EPOC 11 LOSS: 48.76365280151367\n",
      "EPOC 12 LOSS: 36.84973907470703\n",
      "EPOC 13 LOSS: 28.019699096679688\n",
      "EPOC 14 LOSS: 22.16898536682129\n",
      "EPOC 15 LOSS: 18.947093963623047\n",
      "EPOC 16 LOSS: 17.843278884887695\n",
      "EPOC 17 LOSS: 18.17408561706543\n",
      "EPOC 18 LOSS: 19.26832389831543\n",
      "EPOC 19 LOSS: 20.587020874023438\n",
      "MODEL: MF_UI LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 21.763887405395508\n",
      "EPOC 1 LOSS: 16.065977096557617\n",
      "EPOC 2 LOSS: 11.296177864074707\n",
      "EPOC 3 LOSS: 7.530053615570068\n",
      "EPOC 4 LOSS: 4.9186272621154785\n",
      "EPOC 5 LOSS: 3.578477144241333\n",
      "EPOC 6 LOSS: 3.208162784576416\n",
      "EPOC 7 LOSS: 3.272204875946045\n",
      "EPOC 8 LOSS: 3.31290864944458\n",
      "EPOC 9 LOSS: 3.1536407470703125\n",
      "EPOC 10 LOSS: 2.814467430114746\n",
      "EPOC 11 LOSS: 2.3632736206054688\n",
      "EPOC 12 LOSS: 1.882720708847046\n",
      "EPOC 13 LOSS: 1.4370856285095215\n",
      "EPOC 14 LOSS: 1.0660346746444702\n",
      "EPOC 15 LOSS: 0.8050018548965454\n",
      "EPOC 16 LOSS: 0.640828549861908\n",
      "EPOC 17 LOSS: 0.556065022945404\n",
      "EPOC 18 LOSS: 0.5117799639701843\n",
      "EPOC 19 LOSS: 0.48103097081184387\n",
      "MODEL: MF_UI LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.45436570048332214\n",
      "EPOC 1 LOSS: 0.3624366223812103\n",
      "EPOC 2 LOSS: 0.305399090051651\n",
      "EPOC 3 LOSS: 0.26976269483566284\n",
      "EPOC 4 LOSS: 0.24602966010570526\n",
      "EPOC 5 LOSS: 0.22854609787464142\n",
      "EPOC 6 LOSS: 0.2149747610092163\n",
      "EPOC 7 LOSS: 0.2042446732521057\n",
      "EPOC 8 LOSS: 0.19621913135051727\n",
      "EPOC 9 LOSS: 0.19094789028167725\n",
      "EPOC 10 LOSS: 0.18706519901752472\n",
      "EPOC 11 LOSS: 0.18286408483982086\n",
      "EPOC 12 LOSS: 0.17692182958126068\n",
      "EPOC 13 LOSS: 0.16938520967960358\n",
      "EPOC 14 LOSS: 0.16204142570495605\n",
      "EPOC 15 LOSS: 0.1562792807817459\n",
      "EPOC 16 LOSS: 0.1517697274684906\n",
      "EPOC 17 LOSS: 0.1479317545890808\n",
      "EPOC 18 LOSS: 0.14445219933986664\n",
      "EPOC 19 LOSS: 0.14120352268218994\n",
      "EPOC 20 LOSS: 0.1380992978811264\n",
      "EPOC 21 LOSS: 0.13506494462490082\n",
      "EPOC 22 LOSS: 0.13206058740615845\n",
      "EPOC 23 LOSS: 0.12905699014663696\n",
      "EPOC 24 LOSS: 0.12602730095386505\n",
      "MODEL: MF_UI LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.12298337370157242\n",
      "EPOC 1 LOSS: 0.12170232087373734\n",
      "EPOC 2 LOSS: 0.12057724595069885\n",
      "EPOC 3 LOSS: 0.11956889182329178\n",
      "EPOC 4 LOSS: 0.11864949017763138\n",
      "EPOC 5 LOSS: 0.1178002804517746\n",
      "EPOC 6 LOSS: 0.11700861901044846\n",
      "EPOC 7 LOSS: 0.11626593768596649\n",
      "EPOC 8 LOSS: 0.11556575447320938\n",
      "EPOC 9 LOSS: 0.11490236967802048\n",
      "EPOC 10 LOSS: 0.11426999419927597\n",
      "EPOC 11 LOSS: 0.11366164684295654\n",
      "EPOC 12 LOSS: 0.11307020485401154\n",
      "EPOC 13 LOSS: 0.1124901995062828\n",
      "EPOC 14 LOSS: 0.11191821843385696\n",
      "EPOC 15 LOSS: 0.11135248094797134\n",
      "EPOC 16 LOSS: 0.11079251021146774\n",
      "EPOC 17 LOSS: 0.11023838818073273\n",
      "EPOC 18 LOSS: 0.10969069600105286\n",
      "EPOC 19 LOSS: 0.10915021598339081\n",
      "EPOC 20 LOSS: 0.10861754417419434\n",
      "EPOC 21 LOSS: 0.10809284448623657\n",
      "EPOC 22 LOSS: 0.10757562518119812\n",
      "EPOC 23 LOSS: 0.10706524550914764\n",
      "EPOC 24 LOSS: 0.10656071454286575\n",
      "MF_UI validation NDCG: 0.23659287130098428\n",
      "\n",
      "\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 468.3209533691406\n",
      "EPOC 1 LOSS: 404.44091796875\n",
      "EPOC 2 LOSS: 345.6116027832031\n",
      "EPOC 3 LOSS: 291.9976806640625\n",
      "EPOC 4 LOSS: 243.7147674560547\n",
      "EPOC 5 LOSS: 200.88482666015625\n",
      "EPOC 6 LOSS: 163.48959350585938\n",
      "EPOC 7 LOSS: 131.3601531982422\n",
      "EPOC 8 LOSS: 104.1494140625\n",
      "EPOC 9 LOSS: 81.3755874633789\n",
      "EPOC 10 LOSS: 62.575016021728516\n",
      "EPOC 11 LOSS: 47.329803466796875\n",
      "EPOC 12 LOSS: 35.32526779174805\n",
      "EPOC 13 LOSS: 26.3978328704834\n",
      "EPOC 14 LOSS: 20.45931053161621\n",
      "EPOC 15 LOSS: 17.16293716430664\n",
      "EPOC 16 LOSS: 16.000076293945312\n",
      "EPOC 17 LOSS: 16.262622833251953\n",
      "EPOC 18 LOSS: 17.27281379699707\n",
      "EPOC 19 LOSS: 18.48163414001465\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 19.537302017211914\n",
      "EPOC 1 LOSS: 13.823548316955566\n",
      "EPOC 2 LOSS: 9.085287094116211\n",
      "EPOC 3 LOSS: 5.446468830108643\n",
      "EPOC 4 LOSS: 3.1078505516052246\n",
      "EPOC 5 LOSS: 2.1241776943206787\n",
      "EPOC 6 LOSS: 2.021401882171631\n",
      "EPOC 7 LOSS: 2.2122514247894287\n",
      "EPOC 8 LOSS: 2.3341968059539795\n",
      "EPOC 9 LOSS: 2.2765512466430664\n",
      "EPOC 10 LOSS: 2.064318895339966\n",
      "EPOC 11 LOSS: 1.7539681196212769\n",
      "EPOC 12 LOSS: 1.4118056297302246\n",
      "EPOC 13 LOSS: 1.0884673595428467\n",
      "EPOC 14 LOSS: 0.8235247135162354\n",
      "EPOC 15 LOSS: 0.6370352506637573\n",
      "EPOC 16 LOSS: 0.5249490737915039\n",
      "EPOC 17 LOSS: 0.4704204201698303\n",
      "EPOC 18 LOSS: 0.44780483841896057\n",
      "EPOC 19 LOSS: 0.44100257754325867\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.44095686078071594\n",
      "EPOC 1 LOSS: 0.38547757267951965\n",
      "EPOC 2 LOSS: 0.3479301333427429\n",
      "EPOC 3 LOSS: 0.3194906711578369\n",
      "EPOC 4 LOSS: 0.29578647017478943\n",
      "EPOC 5 LOSS: 0.2749519944190979\n",
      "EPOC 6 LOSS: 0.25629058480262756\n",
      "EPOC 7 LOSS: 0.23962391912937164\n",
      "EPOC 8 LOSS: 0.22519591450691223\n",
      "EPOC 9 LOSS: 0.21321962773799896\n",
      "EPOC 10 LOSS: 0.20372579991817474\n",
      "EPOC 11 LOSS: 0.19609402120113373\n",
      "EPOC 12 LOSS: 0.18966077268123627\n",
      "EPOC 13 LOSS: 0.18264102935791016\n",
      "EPOC 14 LOSS: 0.1738303303718567\n",
      "EPOC 15 LOSS: 0.16513600945472717\n",
      "EPOC 16 LOSS: 0.15823319554328918\n",
      "EPOC 17 LOSS: 0.1528463363647461\n",
      "EPOC 18 LOSS: 0.14820371568202972\n",
      "EPOC 19 LOSS: 0.1438111960887909\n",
      "EPOC 20 LOSS: 0.13945817947387695\n",
      "EPOC 21 LOSS: 0.13515834510326385\n",
      "EPOC 22 LOSS: 0.1309390813112259\n",
      "EPOC 23 LOSS: 0.12672735750675201\n",
      "EPOC 24 LOSS: 0.1225636750459671\n",
      "MODEL: MF_UI_BIAS LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.1185612604022026\n",
      "EPOC 1 LOSS: 0.11724963784217834\n",
      "EPOC 2 LOSS: 0.1161247119307518\n",
      "EPOC 3 LOSS: 0.11513827741146088\n",
      "EPOC 4 LOSS: 0.11425131559371948\n",
      "EPOC 5 LOSS: 0.11343647539615631\n",
      "EPOC 6 LOSS: 0.11267606914043427\n",
      "EPOC 7 LOSS: 0.11195816099643707\n",
      "EPOC 8 LOSS: 0.11127389222383499\n",
      "EPOC 9 LOSS: 0.11061596870422363\n",
      "EPOC 10 LOSS: 0.10997732728719711\n",
      "EPOC 11 LOSS: 0.10935143381357193\n",
      "EPOC 12 LOSS: 0.10873227566480637\n",
      "EPOC 13 LOSS: 0.10811589658260345\n",
      "EPOC 14 LOSS: 0.10750086605548859\n",
      "EPOC 15 LOSS: 0.10688770562410355\n",
      "EPOC 16 LOSS: 0.10627821087837219\n",
      "EPOC 17 LOSS: 0.10567459464073181\n",
      "EPOC 18 LOSS: 0.10507849603891373\n",
      "EPOC 19 LOSS: 0.10449061542749405\n",
      "EPOC 20 LOSS: 0.1039106622338295\n",
      "EPOC 21 LOSS: 0.103337861597538\n",
      "EPOC 22 LOSS: 0.10277128219604492\n",
      "EPOC 23 LOSS: 0.1022101491689682\n",
      "EPOC 24 LOSS: 0.10165387392044067\n",
      "MF_UI_BIAS validation NDCG: 0.2418294674064327\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# looping over different values for the ratio (we have to be careful not to \n",
    "# make steps that are too big because if our data is too big it can crash\n",
    "# the session)\n",
    "for negative_ratio in [7,10,15,17]:\n",
    "  # initialize the model with pretrained user and item\n",
    "  model_MF_UI = MF_UI(n_users, n_items, 256)\n",
    "  criterion_MF_UI = nn.BCEWithLogitsLoss()\n",
    "  # initialize the model with pretrained user, item and biases\n",
    "  model_MF_UI_BIAS = MF_UI_BIAS(n_users, n_items, 256)\n",
    "  criterion_MF_UI_BIAS = nn.BCEWithLogitsLoss()\n",
    "  # we store our models in the same dictionary we were working on with the\n",
    "  # basic models\n",
    "  model_dict['MF_UI'] = [model_MF_UI, criterion_MF_UI]\n",
    "  model_dict['MF_UI_BIAS'] = [model_MF_UI_BIAS, criterion_MF_UI_BIAS]\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  # the train_evaluate function will iterate over the 2 models, train and\n",
    "  # evaluate and we are going to be able to see the results in the scores_df\n",
    "  train_evaluate_models(['MF_UI', 'MF_UI_BIAS'], negative_ratio, len(scores_df), wd = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVqZFnMK_1s4"
   },
   "source": [
    "### Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "NYxoLT1t862k",
    "outputId": "38d79ce3-991a-41ad-d39b-53a03495b95c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>negative_samples_ratio</th>\n",
       "      <th>NDGC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GMF</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.061684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MF</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.079096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.060017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MF_UI</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.214617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MF_UI_LINK</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.214956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MF_UI</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.205384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MF_UI_BIAS</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.216861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MF_UI</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.229761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MF_UI_BIAS</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.231508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MF_UI</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.235472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MF_UI_BIAS</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.237491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MF_UI</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.236593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MF_UI_BIAS</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.241829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  negative_samples_ratio      NDGC\n",
       "0          GMF                     5.0  0.061684\n",
       "1           MF                     5.0  0.079096\n",
       "2          MLP                     5.0  0.060017\n",
       "3        MF_UI                     7.0  0.214617\n",
       "4   MF_UI_LINK                     7.0  0.214956\n",
       "5        MF_UI                     7.0  0.205384\n",
       "6   MF_UI_BIAS                     7.0  0.216861\n",
       "7        MF_UI                    10.0  0.229761\n",
       "8   MF_UI_BIAS                    10.0  0.231508\n",
       "9        MF_UI                    15.0  0.235472\n",
       "10  MF_UI_BIAS                    15.0  0.237491\n",
       "11       MF_UI                    17.0  0.236593\n",
       "12  MF_UI_BIAS                    17.0  0.241829"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxgOx8PN__pC"
   },
   "source": [
    "We can see how the MF_UI_BIAS peforms always slightly better than its MF_UI counterpart and the performance increases as negative_samples_ratio increases. \n",
    "\n",
    "Unfortunately, negative_samples_ratio cannot be increased further than that unless we want to work with batches, which I already tried and got worse performances, so we will keep it that way.\n",
    "\n",
    "So, we will choose the model with the highest performance, which is MF_UI_BIAS with negative_samples_ratio = 17 and play around with the regularization parameter Weight Decay, to see if we can further improve our performance.\n",
    "Basically the weight of decay is an implementation of the L2 regularization so it can be helpful to prevent overfiting. With larger values for wd we will be imposing a larger penalty to the model, reducing the size of its coefficients. With smaller values of wd we would be allowing the model to be more flexible, imposing no penalty at all when wd=0.\n",
    "\n",
    "### Regularization for MF with biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "yahR-j2_E3Oj",
    "outputId": "a4a7cefd-bd5a-4316-b4f9-14de29c9169e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>negative_samples_ratio</th>\n",
       "      <th>wd</th>\n",
       "      <th>NDGC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GMF</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MF</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MF_UI</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.214617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MF_UI_LINK</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.214956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  negative_samples_ratio  wd      NDGC\n",
       "0         GMF                     5.0   0  0.061684\n",
       "1          MF                     5.0   0  0.079096\n",
       "2         MLP                     5.0   0  0.060017\n",
       "3       MF_UI                     7.0   0  0.214617\n",
       "4  MF_UI_LINK                     7.0   0  0.214956"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding weight decay column to the scores data frame, until now it has always\n",
    "# been 0\n",
    "scores_df['wd'] = 0\n",
    "# reordering the columns\n",
    "scores_df = scores_df.loc[:, ['Model', 'negative_samples_ratio', 'wd', 'NDGC']]\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5OwLo0kCsdT",
    "outputId": "eb11a173-e399-4527-fd55-d9c43891a47f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: MF_UI_BIAS_WD_1 LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 468.7370910644531\n",
      "EPOC 1 LOSS: 404.8341064453125\n",
      "EPOC 2 LOSS: 345.98126220703125\n",
      "EPOC 3 LOSS: 292.3467102050781\n",
      "EPOC 4 LOSS: 244.04315185546875\n",
      "EPOC 5 LOSS: 201.18739318847656\n",
      "EPOC 6 LOSS: 163.76315307617188\n",
      "EPOC 7 LOSS: 131.60537719726562\n",
      "EPOC 8 LOSS: 104.37188720703125\n",
      "EPOC 9 LOSS: 81.5759048461914\n",
      "EPOC 10 LOSS: 62.75010299682617\n",
      "EPOC 11 LOSS: 47.480194091796875\n",
      "EPOC 12 LOSS: 35.44874954223633\n",
      "EPOC 13 LOSS: 26.493160247802734\n",
      "EPOC 14 LOSS: 20.522741317749023\n",
      "EPOC 15 LOSS: 17.191877365112305\n",
      "EPOC 16 LOSS: 16.00737762451172\n",
      "EPOC 17 LOSS: 16.262903213500977\n",
      "EPOC 18 LOSS: 17.281593322753906\n",
      "EPOC 19 LOSS: 18.493213653564453\n",
      "MODEL: MF_UI_BIAS_WD_1 LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 19.546798706054688\n",
      "EPOC 1 LOSS: 13.831331253051758\n",
      "EPOC 2 LOSS: 9.086775779724121\n",
      "EPOC 3 LOSS: 5.431343078613281\n",
      "EPOC 4 LOSS: 3.096982002258301\n",
      "EPOC 5 LOSS: 2.111593723297119\n",
      "EPOC 6 LOSS: 2.0245373249053955\n",
      "EPOC 7 LOSS: 2.2192471027374268\n",
      "EPOC 8 LOSS: 2.3367533683776855\n",
      "EPOC 9 LOSS: 2.2730705738067627\n",
      "EPOC 10 LOSS: 2.0557544231414795\n",
      "EPOC 11 LOSS: 1.7441651821136475\n",
      "EPOC 12 LOSS: 1.403428316116333\n",
      "EPOC 13 LOSS: 1.0824050903320312\n",
      "EPOC 14 LOSS: 0.8213894367218018\n",
      "EPOC 15 LOSS: 0.6366659998893738\n",
      "EPOC 16 LOSS: 0.526019275188446\n",
      "EPOC 17 LOSS: 0.4719628095626831\n",
      "EPOC 18 LOSS: 0.4496892988681793\n",
      "EPOC 19 LOSS: 0.4430324137210846\n",
      "MODEL: MF_UI_BIAS_WD_1 LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.442092627286911\n",
      "EPOC 1 LOSS: 0.3863305151462555\n",
      "EPOC 2 LOSS: 0.34849292039871216\n",
      "EPOC 3 LOSS: 0.31980636715888977\n",
      "EPOC 4 LOSS: 0.2959965765476227\n",
      "EPOC 5 LOSS: 0.275177925825119\n",
      "EPOC 6 LOSS: 0.25660693645477295\n",
      "EPOC 7 LOSS: 0.23997920751571655\n",
      "EPOC 8 LOSS: 0.2253342866897583\n",
      "EPOC 9 LOSS: 0.2131154090166092\n",
      "EPOC 10 LOSS: 0.2036009430885315\n",
      "EPOC 11 LOSS: 0.19608314335346222\n",
      "EPOC 12 LOSS: 0.18967586755752563\n",
      "EPOC 13 LOSS: 0.1826060563325882\n",
      "EPOC 14 LOSS: 0.173884317278862\n",
      "EPOC 15 LOSS: 0.1653471738100052\n",
      "EPOC 16 LOSS: 0.15842409431934357\n",
      "EPOC 17 LOSS: 0.15293556451797485\n",
      "EPOC 18 LOSS: 0.14815418422222137\n",
      "EPOC 19 LOSS: 0.14367420971393585\n",
      "EPOC 20 LOSS: 0.13932667672634125\n",
      "EPOC 21 LOSS: 0.13507099449634552\n",
      "EPOC 22 LOSS: 0.13087037205696106\n",
      "EPOC 23 LOSS: 0.12666571140289307\n",
      "EPOC 24 LOSS: 0.12251187860965729\n",
      "MODEL: MF_UI_BIAS_WD_1 LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.11848628520965576\n",
      "EPOC 1 LOSS: 0.11717523634433746\n",
      "EPOC 2 LOSS: 0.11604852974414825\n",
      "EPOC 3 LOSS: 0.1150590032339096\n",
      "EPOC 4 LOSS: 0.11417333781719208\n",
      "EPOC 5 LOSS: 0.11336323618888855\n",
      "EPOC 6 LOSS: 0.11260754615068436\n",
      "EPOC 7 LOSS: 0.11189331859350204\n",
      "EPOC 8 LOSS: 0.11121237277984619\n",
      "EPOC 9 LOSS: 0.11055693030357361\n",
      "EPOC 10 LOSS: 0.10991942882537842\n",
      "EPOC 11 LOSS: 0.10929401963949203\n",
      "EPOC 12 LOSS: 0.10867568850517273\n",
      "EPOC 13 LOSS: 0.1080608069896698\n",
      "EPOC 14 LOSS: 0.10744798928499222\n",
      "EPOC 15 LOSS: 0.10683784633874893\n",
      "EPOC 16 LOSS: 0.1062319278717041\n",
      "EPOC 17 LOSS: 0.10563185811042786\n",
      "EPOC 18 LOSS: 0.10503886640071869\n",
      "EPOC 19 LOSS: 0.10445340722799301\n",
      "EPOC 20 LOSS: 0.10387537628412247\n",
      "EPOC 21 LOSS: 0.10330421477556229\n",
      "EPOC 22 LOSS: 0.10273918509483337\n",
      "EPOC 23 LOSS: 0.10217946767807007\n",
      "EPOC 24 LOSS: 0.10162439942359924\n",
      "MF_UI_BIAS_WD_1 validation NDCG: 0.24007991676582152\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: MF_UI_BIAS_WD_2 LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 468.5909423828125\n",
      "EPOC 1 LOSS: 404.69482421875\n",
      "EPOC 2 LOSS: 345.84820556640625\n",
      "EPOC 3 LOSS: 292.2190856933594\n",
      "EPOC 4 LOSS: 243.92327880859375\n",
      "EPOC 5 LOSS: 201.076416015625\n",
      "EPOC 6 LOSS: 163.660400390625\n",
      "EPOC 7 LOSS: 131.51089477539062\n",
      "EPOC 8 LOSS: 104.28237915039062\n",
      "EPOC 9 LOSS: 81.49382781982422\n",
      "EPOC 10 LOSS: 62.678443908691406\n",
      "EPOC 11 LOSS: 47.4193000793457\n",
      "EPOC 12 LOSS: 35.4023323059082\n",
      "EPOC 13 LOSS: 26.468406677246094\n",
      "EPOC 14 LOSS: 20.515199661254883\n",
      "EPOC 15 LOSS: 17.205604553222656\n",
      "EPOC 16 LOSS: 16.034231185913086\n",
      "EPOC 17 LOSS: 16.286767959594727\n",
      "EPOC 18 LOSS: 17.29694938659668\n",
      "EPOC 19 LOSS: 18.506542205810547\n",
      "MODEL: MF_UI_BIAS_WD_2 LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 19.5560359954834\n",
      "EPOC 1 LOSS: 13.843720436096191\n",
      "EPOC 2 LOSS: 9.09827709197998\n",
      "EPOC 3 LOSS: 5.453711032867432\n",
      "EPOC 4 LOSS: 3.1190061569213867\n",
      "EPOC 5 LOSS: 2.1260550022125244\n",
      "EPOC 6 LOSS: 2.0282719135284424\n",
      "EPOC 7 LOSS: 2.220259428024292\n",
      "EPOC 8 LOSS: 2.340033531188965\n",
      "EPOC 9 LOSS: 2.2814347743988037\n",
      "EPOC 10 LOSS: 2.0676698684692383\n",
      "EPOC 11 LOSS: 1.7566922903060913\n",
      "EPOC 12 LOSS: 1.413533091545105\n",
      "EPOC 13 LOSS: 1.0905210971832275\n",
      "EPOC 14 LOSS: 0.8264233469963074\n",
      "EPOC 15 LOSS: 0.6389872431755066\n",
      "EPOC 16 LOSS: 0.5265161395072937\n",
      "EPOC 17 LOSS: 0.4712759852409363\n",
      "EPOC 18 LOSS: 0.44915512204170227\n",
      "EPOC 19 LOSS: 0.44273704290390015\n",
      "MODEL: MF_UI_BIAS_WD_2 LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.442460834980011\n",
      "EPOC 1 LOSS: 0.3869020342826843\n",
      "EPOC 2 LOSS: 0.34912753105163574\n",
      "EPOC 3 LOSS: 0.3205278813838959\n",
      "EPOC 4 LOSS: 0.2966337502002716\n",
      "EPOC 5 LOSS: 0.27564823627471924\n",
      "EPOC 6 LOSS: 0.25700071454048157\n",
      "EPOC 7 LOSS: 0.2403014451265335\n",
      "EPOC 8 LOSS: 0.225723996758461\n",
      "EPOC 9 LOSS: 0.21357664465904236\n",
      "EPOC 10 LOSS: 0.20410902798175812\n",
      "EPOC 11 LOSS: 0.19657659530639648\n",
      "EPOC 12 LOSS: 0.19005098938941956\n",
      "EPOC 13 LOSS: 0.18276597559452057\n",
      "EPOC 14 LOSS: 0.1739029437303543\n",
      "EPOC 15 LOSS: 0.16532254219055176\n",
      "EPOC 16 LOSS: 0.15842020511627197\n",
      "EPOC 17 LOSS: 0.15302249789237976\n",
      "EPOC 18 LOSS: 0.14835496246814728\n",
      "EPOC 19 LOSS: 0.14396552741527557\n",
      "EPOC 20 LOSS: 0.1396491676568985\n",
      "EPOC 21 LOSS: 0.13537606596946716\n",
      "EPOC 22 LOSS: 0.13114166259765625\n",
      "EPOC 23 LOSS: 0.12686724960803986\n",
      "EPOC 24 LOSS: 0.12260770797729492\n",
      "MODEL: MF_UI_BIAS_WD_2 LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.11851833760738373\n",
      "EPOC 1 LOSS: 0.11722956597805023\n",
      "EPOC 2 LOSS: 0.1161184087395668\n",
      "EPOC 3 LOSS: 0.11513649672269821\n",
      "EPOC 4 LOSS: 0.11425046622753143\n",
      "EPOC 5 LOSS: 0.1134360209107399\n",
      "EPOC 6 LOSS: 0.11267555505037308\n",
      "EPOC 7 LOSS: 0.1119585782289505\n",
      "EPOC 8 LOSS: 0.11127676069736481\n",
      "EPOC 9 LOSS: 0.11062135547399521\n",
      "EPOC 10 LOSS: 0.10998336225748062\n",
      "EPOC 11 LOSS: 0.10935486108064651\n",
      "EPOC 12 LOSS: 0.10873060673475266\n",
      "EPOC 13 LOSS: 0.10810860991477966\n",
      "EPOC 14 LOSS: 0.10748934745788574\n",
      "EPOC 15 LOSS: 0.10687445849180222\n",
      "EPOC 16 LOSS: 0.10626599192619324\n",
      "EPOC 17 LOSS: 0.10566520690917969\n",
      "EPOC 18 LOSS: 0.10507238656282425\n",
      "EPOC 19 LOSS: 0.10448716580867767\n",
      "EPOC 20 LOSS: 0.10390894114971161\n",
      "EPOC 21 LOSS: 0.10333701968193054\n",
      "EPOC 22 LOSS: 0.10277072340250015\n",
      "EPOC 23 LOSS: 0.10220959782600403\n",
      "EPOC 24 LOSS: 0.10165320336818695\n",
      "MF_UI_BIAS_WD_2 validation NDCG: 0.238831839191419\n",
      "\n",
      "\n",
      "MODEL: MF_UI_BIAS_WD_3 LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 468.9722595214844\n",
      "EPOC 1 LOSS: 405.0441589355469\n",
      "EPOC 2 LOSS: 346.16644287109375\n",
      "EPOC 3 LOSS: 292.5072937011719\n",
      "EPOC 4 LOSS: 244.17779541015625\n",
      "EPOC 5 LOSS: 201.2953338623047\n",
      "EPOC 6 LOSS: 163.8438720703125\n",
      "EPOC 7 LOSS: 131.66482543945312\n",
      "EPOC 8 LOSS: 104.41307830810547\n",
      "EPOC 9 LOSS: 81.59839630126953\n",
      "EPOC 10 LOSS: 62.75645065307617\n",
      "EPOC 11 LOSS: 47.47238540649414\n",
      "EPOC 12 LOSS: 35.428077697753906\n",
      "EPOC 13 LOSS: 26.46681022644043\n",
      "EPOC 14 LOSS: 20.502553939819336\n",
      "EPOC 15 LOSS: 17.188278198242188\n",
      "EPOC 16 LOSS: 16.012466430664062\n",
      "EPOC 17 LOSS: 16.268842697143555\n",
      "EPOC 18 LOSS: 17.287368774414062\n",
      "EPOC 19 LOSS: 18.503890991210938\n",
      "MODEL: MF_UI_BIAS_WD_3 LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 19.55815315246582\n",
      "EPOC 1 LOSS: 13.850471496582031\n",
      "EPOC 2 LOSS: 9.106535911560059\n",
      "EPOC 3 LOSS: 5.454403400421143\n",
      "EPOC 4 LOSS: 3.108675956726074\n",
      "EPOC 5 LOSS: 2.1064865589141846\n",
      "EPOC 6 LOSS: 2.007540702819824\n",
      "EPOC 7 LOSS: 2.2082762718200684\n",
      "EPOC 8 LOSS: 2.3350589275360107\n",
      "EPOC 9 LOSS: 2.27534556388855\n",
      "EPOC 10 LOSS: 2.0592825412750244\n",
      "EPOC 11 LOSS: 1.7469151020050049\n",
      "EPOC 12 LOSS: 1.4047129154205322\n",
      "EPOC 13 LOSS: 1.0829912424087524\n",
      "EPOC 14 LOSS: 0.8198055028915405\n",
      "EPOC 15 LOSS: 0.6340652108192444\n",
      "EPOC 16 LOSS: 0.5238455533981323\n",
      "EPOC 17 LOSS: 0.468894898891449\n",
      "EPOC 18 LOSS: 0.44699737429618835\n",
      "EPOC 19 LOSS: 0.44030678272247314\n",
      "MODEL: MF_UI_BIAS_WD_3 LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.4404347240924835\n",
      "EPOC 1 LOSS: 0.3855591118335724\n",
      "EPOC 2 LOSS: 0.348105251789093\n",
      "EPOC 3 LOSS: 0.3195575475692749\n",
      "EPOC 4 LOSS: 0.29573503136634827\n",
      "EPOC 5 LOSS: 0.2746838629245758\n",
      "EPOC 6 LOSS: 0.2559162378311157\n",
      "EPOC 7 LOSS: 0.23924840986728668\n",
      "EPOC 8 LOSS: 0.22458869218826294\n",
      "EPOC 9 LOSS: 0.21238332986831665\n",
      "EPOC 10 LOSS: 0.20284709334373474\n",
      "EPOC 11 LOSS: 0.19570225477218628\n",
      "EPOC 12 LOSS: 0.1893000602722168\n",
      "EPOC 13 LOSS: 0.18194176256656647\n",
      "EPOC 14 LOSS: 0.17303019762039185\n",
      "EPOC 15 LOSS: 0.16447122395038605\n",
      "EPOC 16 LOSS: 0.15772630274295807\n",
      "EPOC 17 LOSS: 0.1524949073791504\n",
      "EPOC 18 LOSS: 0.14794975519180298\n",
      "EPOC 19 LOSS: 0.14362506568431854\n",
      "EPOC 20 LOSS: 0.1393585354089737\n",
      "EPOC 21 LOSS: 0.13514506816864014\n",
      "EPOC 22 LOSS: 0.13095730543136597\n",
      "EPOC 23 LOSS: 0.12672874331474304\n",
      "EPOC 24 LOSS: 0.1225273534655571\n",
      "MODEL: MF_UI_BIAS_WD_3 LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.11847344040870667\n",
      "EPOC 1 LOSS: 0.11717886477708817\n",
      "EPOC 2 LOSS: 0.11607066541910172\n",
      "EPOC 3 LOSS: 0.11509963124990463\n",
      "EPOC 4 LOSS: 0.11422807723283768\n",
      "EPOC 5 LOSS: 0.11342798173427582\n",
      "EPOC 6 LOSS: 0.11268199980258942\n",
      "EPOC 7 LOSS: 0.11197863519191742\n",
      "EPOC 8 LOSS: 0.11130867898464203\n",
      "EPOC 9 LOSS: 0.11066335439682007\n",
      "EPOC 10 LOSS: 0.1100345179438591\n",
      "EPOC 11 LOSS: 0.10941534489393234\n",
      "EPOC 12 LOSS: 0.10880132019519806\n",
      "EPOC 13 LOSS: 0.10818977653980255\n",
      "EPOC 14 LOSS: 0.10758006572723389\n",
      "EPOC 15 LOSS: 0.10697340965270996\n",
      "EPOC 16 LOSS: 0.10637183487415314\n",
      "EPOC 17 LOSS: 0.10577719658613205\n",
      "EPOC 18 LOSS: 0.10519048571586609\n",
      "EPOC 19 LOSS: 0.10461167246103287\n",
      "EPOC 20 LOSS: 0.10404007136821747\n",
      "EPOC 21 LOSS: 0.10347474366426468\n",
      "EPOC 22 LOSS: 0.10291485488414764\n",
      "EPOC 23 LOSS: 0.10235965251922607\n",
      "EPOC 24 LOSS: 0.10180869698524475\n",
      "MF_UI_BIAS_WD_3 validation NDCG: 0.24403980106818388\n",
      "\n",
      "\n",
      "MODEL: MF_UI_BIAS_WD_4 LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 468.706298828125\n",
      "EPOC 1 LOSS: 404.7999267578125\n",
      "EPOC 2 LOSS: 345.94488525390625\n",
      "EPOC 3 LOSS: 292.30975341796875\n",
      "EPOC 4 LOSS: 244.00839233398438\n",
      "EPOC 5 LOSS: 201.15818786621094\n",
      "EPOC 6 LOSS: 163.73919677734375\n",
      "EPOC 7 LOSS: 131.59080505371094\n",
      "EPOC 8 LOSS: 104.36785125732422\n",
      "EPOC 9 LOSS: 81.5788345336914\n",
      "EPOC 10 LOSS: 62.75636291503906\n",
      "EPOC 11 LOSS: 47.48733139038086\n",
      "EPOC 12 LOSS: 35.45439147949219\n",
      "EPOC 13 LOSS: 26.486244201660156\n",
      "EPOC 14 LOSS: 20.490400314331055\n",
      "EPOC 15 LOSS: 17.126842498779297\n",
      "EPOC 16 LOSS: 15.891282081604004\n",
      "EPOC 17 LOSS: 16.066404342651367\n",
      "EPOC 18 LOSS: 16.994550704956055\n",
      "EPOC 19 LOSS: 18.117034912109375\n",
      "MODEL: MF_UI_BIAS_WD_4 LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 19.07843589782715\n",
      "EPOC 1 LOSS: 13.47447395324707\n",
      "EPOC 2 LOSS: 8.834887504577637\n",
      "EPOC 3 LOSS: 5.281288146972656\n",
      "EPOC 4 LOSS: 3.028853178024292\n",
      "EPOC 5 LOSS: 2.1017379760742188\n",
      "EPOC 6 LOSS: 2.059812068939209\n",
      "EPOC 7 LOSS: 2.298340082168579\n",
      "EPOC 8 LOSS: 2.4203054904937744\n",
      "EPOC 9 LOSS: 2.3140344619750977\n",
      "EPOC 10 LOSS: 2.0231378078460693\n",
      "EPOC 11 LOSS: 1.6339739561080933\n",
      "EPOC 12 LOSS: 1.2382909059524536\n",
      "EPOC 13 LOSS: 0.9096668362617493\n",
      "EPOC 14 LOSS: 0.6832135915756226\n",
      "EPOC 15 LOSS: 0.5598081946372986\n",
      "EPOC 16 LOSS: 0.5142033696174622\n",
      "EPOC 17 LOSS: 0.5082552433013916\n",
      "EPOC 18 LOSS: 0.5194702744483948\n",
      "EPOC 19 LOSS: 0.5358973145484924\n",
      "MODEL: MF_UI_BIAS_WD_4 LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 0.5516518950462341\n",
      "EPOC 1 LOSS: 0.5038481950759888\n",
      "EPOC 2 LOSS: 0.4635162949562073\n",
      "EPOC 3 LOSS: 0.42607250809669495\n",
      "EPOC 4 LOSS: 0.390318363904953\n",
      "EPOC 5 LOSS: 0.3559560477733612\n",
      "EPOC 6 LOSS: 0.32343366742134094\n",
      "EPOC 7 LOSS: 0.2946356534957886\n",
      "EPOC 8 LOSS: 0.2726181745529175\n",
      "EPOC 9 LOSS: 0.2625930607318878\n",
      "EPOC 10 LOSS: 0.2683684527873993\n",
      "EPOC 11 LOSS: 0.2779596447944641\n",
      "EPOC 12 LOSS: 0.26671990752220154\n",
      "EPOC 13 LOSS: 0.24063797295093536\n",
      "EPOC 14 LOSS: 0.2184516340494156\n",
      "EPOC 15 LOSS: 0.20881180465221405\n",
      "EPOC 16 LOSS: 0.20778676867485046\n",
      "EPOC 17 LOSS: 0.20894265174865723\n",
      "EPOC 18 LOSS: 0.2093496471643448\n",
      "EPOC 19 LOSS: 0.20816004276275635\n",
      "EPOC 20 LOSS: 0.2052803486585617\n",
      "EPOC 21 LOSS: 0.20081375539302826\n",
      "EPOC 22 LOSS: 0.19510924816131592\n",
      "EPOC 23 LOSS: 0.18893016874790192\n",
      "EPOC 24 LOSS: 0.18352970480918884\n",
      "MODEL: MF_UI_BIAS_WD_4 LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 0.1803586483001709\n",
      "EPOC 1 LOSS: 0.17791150510311127\n",
      "EPOC 2 LOSS: 0.17620712518692017\n",
      "EPOC 3 LOSS: 0.17498941719532013\n",
      "EPOC 4 LOSS: 0.17406509816646576\n",
      "EPOC 5 LOSS: 0.17329548299312592\n",
      "EPOC 6 LOSS: 0.17259173095226288\n",
      "EPOC 7 LOSS: 0.17190556228160858\n",
      "EPOC 8 LOSS: 0.17121806740760803\n",
      "EPOC 9 LOSS: 0.17052702605724335\n",
      "EPOC 10 LOSS: 0.1698371320962906\n",
      "EPOC 11 LOSS: 0.1691562533378601\n",
      "EPOC 12 LOSS: 0.16849122941493988\n",
      "EPOC 13 LOSS: 0.16784611344337463\n",
      "EPOC 14 LOSS: 0.16722221672534943\n",
      "EPOC 15 LOSS: 0.166619211435318\n",
      "EPOC 16 LOSS: 0.1660364419221878\n",
      "EPOC 17 LOSS: 0.1654718518257141\n",
      "EPOC 18 LOSS: 0.16492290794849396\n",
      "EPOC 19 LOSS: 0.1643870323896408\n",
      "EPOC 20 LOSS: 0.1638616919517517\n",
      "EPOC 21 LOSS: 0.16334424912929535\n",
      "EPOC 22 LOSS: 0.16283270716667175\n",
      "EPOC 23 LOSS: 0.16232582926750183\n",
      "EPOC 24 LOSS: 0.16182352602481842\n",
      "MF_UI_BIAS_WD_4 validation NDCG: 0.22645078232879648\n",
      "\n",
      "\n",
      "MODEL: MF_UI_BIAS_WD_5 LEARNING RATE = 0.1\n",
      "EPOC 0 LOSS: 468.7234802246094\n",
      "EPOC 1 LOSS: 461.90576171875\n",
      "EPOC 2 LOSS: 445.88189697265625\n",
      "EPOC 3 LOSS: 435.3921813964844\n",
      "EPOC 4 LOSS: 432.0757751464844\n",
      "EPOC 5 LOSS: 433.8115539550781\n",
      "EPOC 6 LOSS: 437.5596618652344\n",
      "EPOC 7 LOSS: 441.1874694824219\n",
      "EPOC 8 LOSS: 443.21112060546875\n",
      "EPOC 9 LOSS: 443.27459716796875\n",
      "EPOC 10 LOSS: 442.01861572265625\n",
      "EPOC 11 LOSS: 440.3295593261719\n",
      "EPOC 12 LOSS: 439.0495910644531\n",
      "EPOC 13 LOSS: 438.7882995605469\n",
      "EPOC 14 LOSS: 439.662353515625\n",
      "EPOC 15 LOSS: 441.3124084472656\n",
      "EPOC 16 LOSS: 443.15606689453125\n",
      "EPOC 17 LOSS: 444.6041259765625\n",
      "EPOC 18 LOSS: 445.24005126953125\n",
      "EPOC 19 LOSS: 444.96630859375\n",
      "MODEL: MF_UI_BIAS_WD_5 LEARNING RATE = 0.05\n",
      "EPOC 0 LOSS: 444.0098571777344\n",
      "EPOC 1 LOSS: 440.15625\n",
      "EPOC 2 LOSS: 441.82000732421875\n",
      "EPOC 3 LOSS: 443.5644226074219\n",
      "EPOC 4 LOSS: 444.2183837890625\n",
      "EPOC 5 LOSS: 443.6495666503906\n",
      "EPOC 6 LOSS: 442.52032470703125\n",
      "EPOC 7 LOSS: 441.4513854980469\n",
      "EPOC 8 LOSS: 440.9315490722656\n",
      "EPOC 9 LOSS: 441.17779541015625\n",
      "EPOC 10 LOSS: 442.01739501953125\n",
      "EPOC 11 LOSS: 443.0725402832031\n",
      "EPOC 12 LOSS: 443.93743896484375\n",
      "EPOC 13 LOSS: 444.29949951171875\n",
      "EPOC 14 LOSS: 444.06182861328125\n",
      "EPOC 15 LOSS: 443.3583984375\n",
      "EPOC 16 LOSS: 442.4643859863281\n",
      "EPOC 17 LOSS: 441.6948547363281\n",
      "EPOC 18 LOSS: 441.29644775390625\n",
      "EPOC 19 LOSS: 441.3573303222656\n",
      "MODEL: MF_UI_BIAS_WD_5 LEARNING RATE = 0.01\n",
      "EPOC 0 LOSS: 441.79620361328125\n",
      "EPOC 1 LOSS: 442.8360900878906\n",
      "EPOC 2 LOSS: 442.92840576171875\n",
      "EPOC 3 LOSS: 442.7605895996094\n",
      "EPOC 4 LOSS: 442.5856628417969\n",
      "EPOC 5 LOSS: 442.52978515625\n",
      "EPOC 6 LOSS: 442.56219482421875\n",
      "EPOC 7 LOSS: 442.639892578125\n",
      "EPOC 8 LOSS: 442.715087890625\n",
      "EPOC 9 LOSS: 442.74017333984375\n",
      "EPOC 10 LOSS: 442.703857421875\n",
      "EPOC 11 LOSS: 442.6260070800781\n",
      "EPOC 12 LOSS: 442.53790283203125\n",
      "EPOC 13 LOSS: 442.4732666015625\n",
      "EPOC 14 LOSS: 442.4559326171875\n",
      "EPOC 15 LOSS: 442.4879455566406\n",
      "EPOC 16 LOSS: 442.55224609375\n",
      "EPOC 17 LOSS: 442.62255859375\n",
      "EPOC 18 LOSS: 442.6723327636719\n",
      "EPOC 19 LOSS: 442.6841735839844\n",
      "EPOC 20 LOSS: 442.6570129394531\n",
      "EPOC 21 LOSS: 442.60589599609375\n",
      "EPOC 22 LOSS: 442.5543212890625\n",
      "EPOC 23 LOSS: 442.524658203125\n",
      "EPOC 24 LOSS: 442.529296875\n",
      "MODEL: MF_UI_BIAS_WD_5 LEARNING RATE = 0.001\n",
      "EPOC 0 LOSS: 442.5663146972656\n",
      "EPOC 1 LOSS: 442.6294250488281\n",
      "EPOC 2 LOSS: 442.65203857421875\n",
      "EPOC 3 LOSS: 442.6445617675781\n",
      "EPOC 4 LOSS: 442.6219177246094\n",
      "EPOC 5 LOSS: 442.5984191894531\n",
      "EPOC 6 LOSS: 442.5809020996094\n",
      "EPOC 7 LOSS: 442.57159423828125\n",
      "EPOC 8 LOSS: 442.5696105957031\n",
      "EPOC 9 LOSS: 442.5723571777344\n",
      "EPOC 10 LOSS: 442.577392578125\n",
      "EPOC 11 LOSS: 442.5827941894531\n",
      "EPOC 12 LOSS: 442.5876159667969\n",
      "EPOC 13 LOSS: 442.59173583984375\n",
      "EPOC 14 LOSS: 442.5955505371094\n",
      "EPOC 15 LOSS: 442.5993957519531\n",
      "EPOC 16 LOSS: 442.6031799316406\n",
      "EPOC 17 LOSS: 442.6063537597656\n",
      "EPOC 18 LOSS: 442.6083068847656\n",
      "EPOC 19 LOSS: 442.6086120605469\n",
      "EPOC 20 LOSS: 442.607177734375\n",
      "EPOC 21 LOSS: 442.6044616699219\n",
      "EPOC 22 LOSS: 442.6013488769531\n",
      "EPOC 23 LOSS: 442.5986633300781\n",
      "EPOC 24 LOSS: 442.5972595214844\n",
      "MF_UI_BIAS_WD_5 validation NDCG: 0.059947135443999364\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculating the performances for different values of weight of decay:\n",
    "wd_values = [1e-12, 1e-10, 1e-8, 1e-6, 0.01]\n",
    "# adding index to be able to store the weight of decay in the scores data frame,\n",
    "# which is not supported by the train_evaluate function\n",
    "j = len(scores_df)\n",
    "wd_model_number = 1\n",
    "for wd in wd_values:\n",
    "  # initialize model\n",
    "  model_MF_UI_BIAS_WD= MF_UI_BIAS(n_users, n_items, 256)\n",
    "  criterion_MF_UI_BIAS_WD = nn.BCEWithLogitsLoss()\n",
    "  model_dict['MF_UI_BIAS_WD'+'_'+ str(wd_model_number)] = [model_MF_UI_BIAS_WD, criterion_MF_UI_BIAS_WD]\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  # negative_samples ratio is fixed in 17 because it corresponded to the \n",
    "  # highest score with bias included\n",
    "  train_evaluate_models(['MF_UI_BIAS_WD'+'_'+ str(wd_model_number)], 17, len(scores_df), wd = wd)\n",
    "  scores_df.loc[j, 'wd'] = wd\n",
    "  j += 1\n",
    "  wd_model_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbeNWxHhAclE"
   },
   "source": [
    "### Final scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "8CgDtUX8N0Rf",
    "outputId": "328e0d63-0302-4b00-f3e1-e7715d773250"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>negative_samples_ratio</th>\n",
       "      <th>wd</th>\n",
       "      <th>NDGC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MF_UI_BIAS_WD_3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.244040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MF_UI_BIAS</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.241829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MF_UI_BIAS_WD_1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.000000e-12</td>\n",
       "      <td>0.240080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MF_UI_BIAS_WD_2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.238832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MF_UI_BIAS</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.237491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MF_UI</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.236593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MF_UI</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.235472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MF_UI_BIAS</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.231508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MF_UI</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.229761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MF_UI_BIAS_WD_4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.226451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MF_UI_BIAS</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.216861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MF_UI_LINK</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.214956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MF_UI</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.214617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MF_UI</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.205384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MF</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.079096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GMF</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.061684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.060017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MF_UI_BIAS_WD_5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.059947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  negative_samples_ratio            wd      NDGC\n",
       "15  MF_UI_BIAS_WD_3                    17.0  1.000000e-08  0.244040\n",
       "12       MF_UI_BIAS                    17.0  0.000000e+00  0.241829\n",
       "13  MF_UI_BIAS_WD_1                    17.0  1.000000e-12  0.240080\n",
       "14  MF_UI_BIAS_WD_2                    17.0  1.000000e-10  0.238832\n",
       "10       MF_UI_BIAS                    15.0  0.000000e+00  0.237491\n",
       "11            MF_UI                    17.0  0.000000e+00  0.236593\n",
       "9             MF_UI                    15.0  0.000000e+00  0.235472\n",
       "8        MF_UI_BIAS                    10.0  0.000000e+00  0.231508\n",
       "7             MF_UI                    10.0  0.000000e+00  0.229761\n",
       "16  MF_UI_BIAS_WD_4                    17.0  1.000000e-06  0.226451\n",
       "6        MF_UI_BIAS                     7.0  0.000000e+00  0.216861\n",
       "4        MF_UI_LINK                     7.0  0.000000e+00  0.214956\n",
       "3             MF_UI                     7.0  0.000000e+00  0.214617\n",
       "5             MF_UI                     7.0  0.000000e+00  0.205384\n",
       "1                MF                     5.0  0.000000e+00  0.079096\n",
       "0               GMF                     5.0  0.000000e+00  0.061684\n",
       "2               MLP                     5.0  0.000000e+00  0.060017\n",
       "17  MF_UI_BIAS_WD_5                    17.0  1.000000e-02  0.059947"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores data frame ordered from highest NDGC to lowest\n",
    "scores_df = scores_df.sort_values(by=['NDGC'], ascending = False)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gPTpyXjAn62"
   },
   "source": [
    "We can see that the best performing model of all is the Matrix Factorization, having included users and items pre-trained features, with a negative_ratio of 17, biases and a weight of decay of 1e-8.\n",
    "\n",
    "### Making predictions\n",
    "\n",
    "Now that we have found our best performing model we can go ahead and make the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0laDroTEaaON"
   },
   "outputs": [],
   "source": [
    "# creating test loader\n",
    "\n",
    "users_tensor_test = torch.tensor(test_data.user_id.tolist())\n",
    "items_tensor_test = torch.tensor(test_data.item_id.tolist())\n",
    "datatest = TensorDataset(users_tensor_test, items_tensor_test)\n",
    "\n",
    "test_loader = DataLoader(dataset=datatest,\n",
    "    # for speed purposes we use large test batch sizes. These will be broken in \n",
    "    # chunks of 100 because we are given 100 instances of each user\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4Q5OCW5S5Rb"
   },
   "outputs": [],
   "source": [
    "# best-performing model\n",
    "model = model_dict['MF_UI_BIAS_WD_3'][0]\n",
    "# data frame that will store the recommendations\n",
    "recommendations_df = pd.DataFrame()\n",
    "i = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        user = data[0]\n",
    "        items = data[1]\n",
    "        if use_cuda:\n",
    "            user, items = user.cuda(), items.cuda()\n",
    "        preds = model(user, items)\n",
    "        items_cpu = items.cpu().numpy()\n",
    "        preds_cpu = preds.detach().cpu().numpy()\n",
    "        litems=np.split(items_cpu, test_loader.batch_size//100)\n",
    "        lpreds=np.split(preds_cpu, test_loader.batch_size//100)\n",
    "        gtitem = items[0]\n",
    "        # the following 3 lines of code ensure that the fact that the 1st item is\n",
    "        # gtitem does not affect the final rank\n",
    "        randidx = np.arange(100)\n",
    "        np.random.shuffle(randidx)\n",
    "        items, preds = np.array(litems[0])[randidx], np.array(lpreds[0])[randidx]\n",
    "        map_item_score = dict( zip(items, preds) )\n",
    "        rank_list = heapq.nlargest(15, map_item_score, key=map_item_score.get)\n",
    "        user_id = user.tolist()[0]\n",
    "        # storing predictions in data frame\n",
    "        for recommended_item in rank_list:\n",
    "          recommendations_df.loc[i, 'user_id'] = user_id\n",
    "          recommendations_df.loc[i, 'item_id'] = recommended_item\n",
    "          i += 1\n",
    "recommendations_df['user_id'] = recommendations_df['user_id'].astype(int)\n",
    "recommendations_df['item_id'] = recommendations_df['item_id'].astype(int)\n",
    "# saving results\n",
    "recommendations_df.to_csv('/content/drive/My Drive/res2021/31240992_bias17_output.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydyuvRm8C48f"
   },
   "source": [
    "## IV. References\n",
    "\n",
    "* James Le (2020). Recommendation System Series Part 4: The 7 Variants of Matrix Factorization For Collaborative Filtering. Retrieved from \n",
    "https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5\n",
    "\n",
    "* GitHub (2021). Matrix Factorization Experiments. Retrieved from https://github.com/khanhnamle1994/MetaRec/tree/master/Matrix-Factorization-Experiments\n",
    "\n",
    "* GitHub (2019). Generalized Matrix Factorization (GMF). Retrieved from https://github.com/jrzaurin/RecoTour/blob/master/Amazon/neural_cf/Chapter02_GMF.ipynb\n",
    "\n",
    "* GitHub (2019). Multi-Layer Perceptron (MLP) approach to Matrix Factorization. Retrieved from https://github.com/jrzaurin/RecoTour/blob/master/Amazon/neural_cf/Chapter03_MLP.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of FIT5212_Ass2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
